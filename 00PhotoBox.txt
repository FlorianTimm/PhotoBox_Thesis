Verfasser
Florian Timm
Matrikelnummer: 6028121
Gaiserstraße 2, 21073 Hamburg
E-Mail: florian.timm@hcu-hamburg.de
Erstprüfer
Prof. Dr.-Ing. Thomas Kersten
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: thomas.kersten@hcu-hamburg.de
Zweitprüfer
Dipl.-Ing. Kay Zobel
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: kay.zobel@hcu-hamburg.de
Kurzzusammenfassung
Die Photogrammetrie bietet die Möglichkeit, mit relativ einfacher Technik 3D-Modelle zu erstellen. Die Aufnahme der Bilder ist jedoch sehr zeitaufwendig und daher für die Erfassung vieler Objekte, z.B. bei der Digitalisierung von Museumsobjekten, nicht praktikabel. Systeme mit mehreren fest installierten Kameras setzen in der Regel auf hochwertige Kameras, die jedoch die Hardwarekosten stark in die Höhe treiben.

In dieser Arbeit soll der Lösungsansatz untersucht werden, mehrere kostengünstige Kameras, die fest auf einem Rahmen montiert sind, zu verwenden. Mit Kameras für den Raspberry Pi soll ein photogrammetrisches Messsystem für kleine Objekte aufgebaut werden. Dazu soll eine Schnittstelle zur Synchronisation der Kameras programmiert und eine Möglichkeit zur Kalibrierung der Kameras entwickelt werden. Das Endergebnis soll es im Idealfall auch einem photogrammetrischen Laien ermöglichen, schnell und ohne lange Einarbeitungszeit 3D-Modelle in akzeptabler Auflösung und Qualität zu erzeugen.
Abstract
Photogrammetry offers the possibility of creating 3D models with relatively simple technology. However, capturing the images is very time consuming and therefore not practical for many objects, such as museum digitisation. Systems using multiple fixed cameras usually rely on high quality cameras, which significantly increases hardware costs.

This paper explores the solution of using multiple low-cost cameras mounted on a frame. Cameras for the Raspberry Pi are used to build a photogrammetric measurement system for small objects. This involves programming an interface to synchronise the cameras and developing a way to calibrate the cameras. The end result should ideally enable even a photogrammetric layman to produce 3D models of acceptable resolution and quality quickly and without a long training period.

Einleitung

Konzept

In Museen besteht vielfach der Wunsch, ihre Exponate zu digitalisieren. Entsprechende Handreichungen des Deutschen Museumsbundes legen auch die Digitalisierung als 3D-Modelle nahe, verweisen aber auf Aufwand und Format-Probleme . Auch bei Ausgrabungen aber auch in anderen Bereichen besteht der Bedarf dreidimensionale Modelle einfach und kostengünstig zu erfassen.

Im Rahmen dieser Arbeit soll ein Kamerasystem basiert auf Raspberry-Pi-Kameras entwickelt und untersucht werden, in wie weit es diesen Anforderungen gerecht wird. Es soll mittels Photogrammetrie mit geringen personellen Aufwand kleine Objekte bis etwa 40 cm Durchmesser erfassen. Die Bedienung soll dabei laiensicher und mit nur kurzer Einarbeitungszeit möglich sein, dass System also die meisten Schritte selbstständig durchführen. Auch der Nachbau soll mit etwas handwerklichen Geschick möglich sein. Um Lizenzkosten zu sparen, soll die Möglichkeit auf OpenSource-Software zu nutzen geprüft werden.

Als Prototyp soll ein System mit 24 Kameras gebaut werden. Neben der eigentlichen Entwicklung und Untersuchung dieses Systemes soll abschließend die Anzahl der Kameras und die Nutzung eines Drehtellers evaluiert werden, um hiermit gegebenenfalls die Hardwarekosten weiter zu senken oder die Auflösung und Genauigkeit zu steigern.

Stand der Thematik

Es gab bereits einige Arbeiten zu ähnlichen Theman. Hauptsächlich unterschieden diese sich in der Wahl der Kameras und der Zielgruppe der Bediener. Häufig wurden hochwertige Kameras verwendet, die jedoch hohe Kosten in der Anschaffung verursachen. Beispielsweise wurde schon 1990 ein System von Leica entwickelt,

Photogrammmetrische Grundlagen

Das zu entwickelnde System soll 3D-Modelle von Objekten erstellen. Hierfür wird Photogrammmetrie in Form einer SfM-Pipeline genutzt. Der allgemeine Ablauf ist in [img:ablauf] dargestellt. Zunächst werden die Bilder aufgenommen (siehe [s:bilder]). Hierbei ist es wichtig, dass die Bildinhalte sich überlappen. Die Bilder werden dann verknüpft, indem identische Punkte in den Bildern identifiziert werden (siehe [s:verknuepfung]). Hierfür können beispielsweise ArUco-Marker oder die SIFT-Methode genutzt werden. Aus den identifizierten Punkten werden dann die Positionen und Ausrichtung der Kameras und Verknüpfungspunkte in einem lokalen Koordinatensystem ohne bekannten Maßstab berechnet werden. Die berechneten Daten werden anschließend in einer Bündelblockausgleichung gemeinsam optimiert (siehe [s:buendelblock]). Durch die Nutzung von bekannten Größen beispielsweise durch Maßstäbe kann dieses System transformiert werden.

Dieses Kapitel beschreibt die hierfür notwendigen Bedingungen und die Grundlagen der Rekonstruktion des Objektes als 3D-Modell.

[]

Ablauf der Bildverknüpfung, nach

Bilder

Die Berechnung der Tiefeninformationen ist nur möglich, sofern der Punkt in mindestens einem weiteren Bild abgebildet ist. Die Genauigkeit der Berechnung ist vom Schnittwinkel dieser beiden Strahlen abhängig. Um möglichst gute Schnitte zur Verfügung zu haben und die innere und äußere Orientierung möglichst gut berechnen zu können, müssen diese Bilder einige Bedingungen erfüllen.

Überlappung und Bildinhalte

Da die Bilder durch identische Punkte verbunden werden, müssen die Bildinhalte sich überlappen. Die automatische Identifikation von identischen Punkten ist auf verschiedene Weisen möglich: Entweder durch die Nutzung von codierten Passpunkten wie ArUco-Markern oder konzentrischen Passpunkten nach . Alternativ können auch eine Merkmalsextraktion zur Indentifikation von gemeinsamen Punkten genutzt werden, beispielsweise durch die SIFT-Methode. Hierfür muss die oberfläche aber genügend Texturen aufweisen.

Belichtung

und bei sich stark ändernden Helligkeitsverhältnissen auch hilfreich, jedoch wird hierdurch auch die Helligkeit der Verknüpfungspunkte verändert, was wiederum problematisch sein kann. Entsprechend ist es empfehlenswert bei gleichmäßiger Beleuchtung, die sich auch nicht ändern sollte, die Bilder zu erstellen - also beispielsweise bei bedeckten Himmel.

Position und Ausrichtung der Kamera

Bilder, die vom gleichen Standpunkt aufgenommen wurden, sind oft nur ungenau verknüpfbar. Daher empfiehlt es sich, Bilder aus verschiedensten Richtungen zu machen, also bei der manuellen Photographie um das Objekt herumzugehen - eine Mehrbildaufnahme im Rundum-Verband zu erzeugen. Entsprechend müssen die Kameras an dem Trägern auch gleichmäßig um das Objekt verteilt positioniert werden und dabei auch an die Form des Objektes wie Einschnitte anpassbar sein.

Innere Orientierung

Aus der Position eines Punktes in einem Bild kann vereinfacht gedacht ähnlich einer Messung mit einem Theodolit die Richtung des Punktes in Relation zu der Kamera bestimmt werden. Damit diese Berechnung möglich wird, müssen die Parameter der Kamera bekannt sein, die sogenannte innere Orientierung. Sie beschreibt die Abbildung der Kamera mathematisch. Wichtigste Parameter sind hierbei die Lage des Bildhauptpunktes und die Kamerakonstante. Außerdem zählen hierzu auch die Parameter, die die Verzeichnung beschreiben.

Die innere Orientierung kann während der Messung beispielsweise mittels Bündelblockausgleichung bestimmt werden. Bedingung hierfür ist jedoch, dass die innere Orientierung stabil ist und sich nicht während der Messung ändert .

Jede Einstellung der Kameraoptik verändert die innere Orientierung und auch jede Kamera, auch einer Modellreihe, kann je nach Genauigkeitsanspruch als unterschiedlich angesehen werden. Änderungen können sich beispielsweise durch Umfokussierung oder die Nutzung eines optischen Zoom ergeben, aber auch durch einen mechanisch instabilen Aufbau der Kameras. Daher sollten die Bilder möglichst mit einer Kamera mit festen Einstellungen (Brennweite, Fokus, Blende, Objektiv) aufgenommen werden. Änderungen der Empfindlichkeit (ISO-Zahl) oder Belichtungszeit sind unproblematisch für die innere Orientierung .

Verknüpfungspunkte

Um die einzelnen Bilder verknüpfen zu können, werden identische Punkte zwischen zwei oder mehr Bildern benötigt. Diese können klassisch per Hand erfasst werden, jedoch ist dieses schon bei kleineren Projekten sehr zeitaufwändig. Daher wurde zusätzlich die Möglichkeit genutzt, automatisch Verknüpfungspunkte zu erzeugen.

Codierte Zielmarker

Es gibt verschiedenste Formen von Markern, die automatisch erfasst werden können. Grob unterschieden werden kann in codierte und nicht codierte Zielmarker. Beispiele für nicht codierte sind beispielsweise einfache kreisförmige Klebepunkte oder Marker, die aus dem Linien bestehen und ihren Mittelpunkt durch dessen Schnitt definieren. Vorteilhaft ist jedoch die Verwendung von codierten Zielmarken. Hier können die Punkte ihren Nummern direkt zugeordnet werden.

ArUco-Marker

Eine Variante der automatischen Verknüpfungspunkte sind die sogenannten ArUco-Marker. Diese werden häufig für die Orientierung bei Augmented-Reality-Anwendungen genutzt. OpenCV unterstützt die Erkennung dieser Marker. Sie werden als codierte Messmarken verwendet und können automatisch im Subpixelbereich erkannt werden. Jede Ecke kann hier einzeln identifiziert werden, sodass ein erkannter Marker vier Verknüpfungspunkte liefern kann.

Zielmarken nach Schneider

SIFT

Die SIFT-Methode liefert Verknüpfungspunkte aus Mustern auf den photographierten Oberflächen. Es ist meist nicht notwendig explizit Marker an dem aufzunehmenden Objekt anzubringen, sofern seine Oberfläche nicht strukturlos ist (glatte weiße Wände etc.) oder in Bewegung ist.

Zur Erkennung von Merkmalen setzt das Verfahren auf die Detektion von Kanten. Diese werden in verschiedenen Stufen einer Bildpyramide erkannt und ihre Extrema berechnet. Es werden diese Merkmale weiter ausgedünnt, beispielsweise über den Kontrast. Sofern ein möglicher Marker identifiziert wurde, wird eine Beschreibung erzeugt. Diese erfolgt durch Analyse der Helligkeitsabweichungen zu den Nachbar-Pixeln und wird an der stärksten Abweichung ausgerichtet. Hierdurch wird die Beschreibung dann richtungsunabhängig. Mit diesen kann dann die Übereinstimmung von zwei Markern in zwei Photos bestimmt werden, auch wenn die Bilder zueinander gekippt oder gedreht sind.

Verknüpfung von Bildern

Durch die beschriebenen Verfahren und die hieraus entstandenen Verknüpfungspunkte können die Bilder miteinander verknüpft werden. Da die Kamerapositionen am Rahmen veränderlich sind und die Fixierung auch keine ausreichend genaue Fixierung garantiert, können die bekannten Positionen aus vorherigen Messungen nur als Näherungswerte genutzt werden. Die genaue Bestimmung erfolgt der Position und Ausrichtung - die äußere Orientierung - erfolgt dann Über ein Photogrammmetrische Verfahren. Dieses wird im Folgenden vorgestellt.

Abbildungsgleichung

Die Abbildung eines Punktes auf einem Bild wird durch die Abbildungsgleichung beschrieben. In der Matrizenrechnung ergibt sich dieser aus der Multiplikation mit der Projektionsmatrix P. Diese ergibt sich aus der Kameramatrix K, der Rotation R und dem Projektionszentrum X₀. (siehe [abbildungsgleichung], nach und )

$$\begin{aligned}
    \label{abbildungsgleichung}
    x' & = P \cdot X       \\
    P  & = K \cdot [R|X_0] \\
    P  & =
    \begin{bmatrix}
        c_x & 0   & x'_0 \\
        0   & c_y & y'_0 \\
        0   & 0   & 1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        r_11 & r_21 & r_31 & X_0 \\
        r_12 & r_22 & r_32 & Y_0 \\
        r_13 & r_23 & r_33 & Z_0 \\
    \end{bmatrix}
\end{aligned}$$

Um die Beziehung zwischen zwei Bildern aufzustellen, kann man die Abbildungsgleichung nutzen. Da es hier nur um die Beziehung zwischen zwei Bildern geht, kann die Rotation und Translation des ersten Bildes auf 0 gesetzt werden (R ist dann eine 3x3-Einheitsmatrix und X₀ ein Nullvektor). X₀ des zweiten Bildes wird zur Translation zwischen den beiden Bildern.

Rückwärtsschnitt

Die Koordinaten der ArUco-Marker sind aus einer Kalibrierung und aus vorherigen Messungen bekannt. Aus diesen und den Bildkoordinaten der Marker kann die Position und Ausrichtung der Kamera berechnet werden. Hierfür wird der sogenannte Rückwärtsschnitt genutzt. Die Berechung erfolgt auf Basis der Abbildungsgleichung, die die Position eines Punktes in einem Bild in Beziehung zur Kamera setzt. Für die Berechnung selbst gibt es verschiedene Methoden. Verwendet wurde hier der von OpenCV genutzte Ansatz von in Kombination mit einem RANSAC-Ansatz. Durch die Nutzung des RANSAC-Ansatzes können veränderte Passpunkte identifiziert und als Ausreißer markiert werden.

Vorwärtsschnitt

Die Koordinaten der Passpunkt-Ausreißer aus der Berechung der Kamerapositionen werden anschließend neu berechnet. Hierfür wird der Vorwärtsschnitt genutzt. Auch hier wird OpenCV zur Berechnung genutzt. Die Berechnung der Passpunkte erfolgt für jedes Bildpaar einzeln. Für alle Koordinaten wird dann pro Passpunkt der Z-Score berechnet. Passpunkte, die einen Z-Score von über 2 haben, werden als Ausreißer markiert und nicht weiter betrachtet.

Mit dem Vorwärtsschnitt können auch die Neupunkte, die mittels SIFT oder ähnlichen Bilderkennungsalgorithmen erkannt wurden, berechnet werden. Dadurch kann schon eine dünne Punktwolke erzeugt werden, die dann in der Bündelblockausgleichung weiter optimiert werden kann.

Bündelblockausgleichung

Mittels Bündelblockausgleichung können die grob mit den vorher genannten Verfahren bestimmten Positionen und Drehungen in einer Ausgleichung optimiert werden. Hierzu gehen alle Parameter der Bilder und die Positionen der Passpunkte in die gemeinsame Ausgleichung ein. Grundlage der Ausgleichung ist die in [ss:abbildungsgleichung] beschriebene Abbildungsgleichung. Als Ergebnis erhält man die ausgeglichenen Parameter und Genauigkeitsangaben für diese.

Multi-view Stereo

Die dünne Punktwolke aus dem Vorwärtsschnitt kann durch Multi-view Stereo-Verfahren zu einem 3D-Modell erweitert werden. Hierbei werden jeweils Bildpaare gebildet und die Disparitäten, also die Verschiebung des Objektes in der Abbildung, bestimmt. Diese sind abhängig von der Entfernung des Objektes, bei einem unendlich weit entfernten Objekt tritt keine Disparität auf . Die Disparitäten können dann in Tiefeninformationen umgerechnet werden. Diese können dann wiederum gemittelt und zu einen Tiefenbild zusammengefasst werden.

Mesh-Generierung

Bis zu diesem Schritt besteht das Modell nur aus einzelnen Punkten, die keine Oberfläche ergeben. Um ein 3D-Modell zu erhalten, muss eine Oberfläche generiert werden. Hierfür wird ein Mesh-Generierungsverfahren genutzt. Hierbei wird die Punktwolke in Dreiecke unterteilt. Hierfür gibt es verschiedene Verfahren, die sich in der Art der Unterteilung unterscheiden. OpenDroneMap nutzt beispielsweise die Screened Poisson Surface Reconstruction.

Die Screened Poisson Surface Reconstruction ist ein Verfahren, das auf der Poisson-Gleichung basiert. Diese wird genutzt, um die Oberfläche zu glätten und zu interpolieren. Ein Teil des Ansatzes ist es, dass die Ausrichtung der Punkte berückstichtigt wird. Die Punkte werden hierbei in eine Gitterstruktur überführt und die Oberfläche durch die Lösung der Poisson-Gleichung bestimmt.

Texturierung

Abschließend wird das Modell texturiert. Hierfür werden die Bilder, die zur Erstellung des Modells genutzt wurden, auf das Modell projiziert. Außerdem werden Helligkeits- und Farbunterschiede ausgeglichenen.

Aufbau des Messsystemes

Die Kameras sollten eine hohe geometrische Auflösung und möglichst stabile innere Orientierung aufweisen. Außerdem sollen sie während einer Messkampagne nicht in ihrer Lage zueinander verändert werden, damit die äußere Orientierung größtenteils unverändert bleibt. Daher ist ein stabiler Rahmen notwendig, an welchem die Kameras verdrehsicher angebracht werden können. Kleinere Restfehler in den Orientierungen können mit ausgeglichen werden. Um Ungenauigkeiten durch Bewegungen zu verhindern, müssen die Kameras möglichst zeitgleich auslösen. Daher ist eine gemeinsame Steuerung und Kommunikation zwischen den Kameras notwendig. Außerdem sollen alle Bilder dann auf das Steuerungssystem übertragen werden, hierfür wir eine Form der Datenübertragung benötigt. Damit die Bilder möglichst schattenfrei ausgeleuchtet werden, muss Beleuchtung mit eingeplant werden. Außerdem muss die Stromversorgung der einzelnen Kameras sichergestellt sein.

Aus diesen Anforderungen ergeben sich die einzelnen Abschnitte dieses Kapitels.

Kameras

Als Kameras wurde das Raspberry Pi Camera Module 3 verwendet, welches jeweils von einem Raspberry Pi Zero W gesteuert wird. Im Vergleich zu anderen günstigen Kameras wie Webcams oder der ESP32 CAM haben die Kameras eine hohe geometrische Auflösung von 12 Megapixeln und dennoch mit 1, 4 μm relativ große Pixel , was im subjektiven Eindruck eine sehr gute Bildqualität ergibt.

Nachteil und Vorteil zugleich ist, dass die Kamera über einen Autofokus verfügt, der aber auch elektronisch gesteuert manuell fokussieren kann. Dieser verschlechtert die Stabilität der inneren Orientierung (vgl. [s:innereorientierung]) weiter und wurde daher auch besonders im analysiert. Da die Bilder aber Nahbereich zwischen 20 und 70 cm benötigt werden, ist hier die Schärfentiefe niedrig. Der elektronische Fokus, eine wiederholgenaue und damit mathematisch modellierbare Fokussierung vorausgesetzt, ermöglicht hier, Fokusstacking zu nutzen um den Schärfebereich zu vergrößern.

Weiterer Vorteil der Lösung mit einzelnen Raspberry Pis ist es, dass hierdurch bereits die einzelnen Kameraeinheiten Berechnungen wie das Identifizieren von Passpunkten übernehmen könnten und auch durch die Nutzung von Netzwerkverbindungen für die Steuerung das System skalierbar im Sinne der Anzahl der Kameras aber auch der Größenordnung der Abstände zwischen den Kameras.

Rahmen

[]

Kamera-Winkel

[]

Aluminum-Rahmen

Rahmen und Kameramontage-Winkel

Der Rahmen muss möglichst stabil sein, damit die Kameras sich nicht in ihrer Lage verändern können. Jedoch sollte das System auch weiterhin transportabel - also nicht zu schwer - und veränderbar bleiben, beispielsweise Kameras für Messreihen in ihrer Lage verändert werden. Der Aufbau aus genormten Bauteilen bietet sich an, um hier ggf. den Nachbau einfach ermöglichen zu können. Außerdem sollte der Rahmen auch gegebenenfalls auseinandernehmbar sein, damit er transportiert werden kann.

Als mögliche Materialien kamen Holz, Stahl und Aluminum in Frage. Aufgrund der einfachen Bearbeitung und der genormten Profile, wurde sich für Aluminiumprofile entschieden. Diese gibt es in verschiedenen Ausführungen mit Nuten an den Seitenflächen, so dass eine einfache Montage, aber auch eine Demontage zu Transportzwecken, möglich wird. Außerdem sind diese sehr stabil bei leichtem Gewicht. Durch eine Konstruktion mit Eckwürfeln sowie dem Einbau von dreieckigen Strukturen und Platten, die Scheibenwirkung haben, wurde die Stabilität der Verbindungen erhöht. [img:alurahmen] zeigt den fertigen Rahmen vor Einbau der Platten und der Technik.

Die Kameras wurden mit einem 90^(∘)-Winkel montiert, um diese weiterhin noch vertikal schwenken zu können. [img:aluwinkel] zeigt einen der Winkel, der an den Aluprofilen befestigt ist (noch ohne entsprechende Befestigungsbohrungen).

Beleuchtung

[]

Schattenarme Beleuchtung durch LED-Streifen

[]

Farbige Beleuchtung zur Statusmeldung

Beleuchtung

Um möglichst gute Bilder zu erzeugen, sollte das Objekt gut ausgeleuchtet sein. Eine dunkle Umgebung verlängert die Belichtungszeit, wodurch die Gefahr von unscharfen Aufnahmen steigt. Dunkele Bereiche (ungleichmäßige Ausleuchtung) verursachen verstärktes Rauschen in diesen Bildbereichen. Daher soll das System eine gleichmäßige Ausleuchtung ermöglichen. Problematisch ist hierbei, dass die Kameras ggf. auch die Lichtquellen mit im Bildbereich haben können, wodurch Linsenreflexionen oder Ausbrennen der Bildbereiche möglich sind. Außerdem störend sind fremde Lichtquellen, die Schatten werfen oder die Belichtung der Kameras beeinflussen können.

Es wurde sich zur Beleuchtung für einzeln steuerbare LED-Lichtstreifen entschieden (siehe [img:led_streifen]). Diese können einfach an den Aluprofilen montiert werden und ermöglichen es, einzelne Bereiche und so ggf. blendende Bereiche abzuschalten. Außerdem können hiermit auch verschiedene Lichtfarben eingestellt werden, um Statusmeldungen zu geben (siehe [img:led_farbig]) oder ggf. die Farbgebung des Objektes zu beeinflussen. Die Steuerung erfolgt über einen Raspberry Pi 4, der auch die Steuerung der Kameras übernimmt.

Für diffuseres Licht von außen kann ein halbtransparenter, weißer Stoff über den Rahmen gespannt werden. Dieser sorgt für eine gleichmäßigere Ausleuchtung durch Reflektion im inneren, verhindert auch Reflexionen an Glasscheiben und ähnlichem und vermindert Blendwirkungen durch externe Lichtquellen. Hierfür wurde aus weißem Baumwollstoff eine entsprechende Haube genäht, welche durch zwei mittels Reißverschluss verschließbaren Eingriffmöglichkeiten weiterhin das Einlegen von Objekten durch die Seite oder das Verstellen von Kameras ermöglicht.

Stromversorgung

[]

Elektroverteilung zu den einzelnen Raspberry Pi Zero

Die Raspberry Pis werden mit 5 Volt betrieben. Der Raspberry Pi Zero mit Kamera hatte dabei in Messungen einen maximalen Stromverbrauch von 270 mA aufgezeigt, der Raspberry Pi 4 kann bis zu 1,5 A unter Last verbrauchen. Hieraus ergibt sich ein Gesamtstromverbrauch von maximal rund 8 Ampere. Für den Raspbery Pi 4 wurde ein eigenes Netzteil eingeplant und für die Zero W ein gemeinsames 35 Watt-Netzteil. Versuche zeigten jedoch, dass der Stromverbrauch kurzfristig höher ausfallen konnte, so dass die Zero W, die am meisten von Spannungsabfällen betroffen sind zum Absturz gebracht wurden, wenn alle Kameras gleichzeitig auslösten. Nach dem die Last auf sicherheitshalber auf zwei weitere Netzteile verteilt wurde, lief das System zuverlässig.

Als Kabelmaterial wurde Klingeldraht mit 0, 75 mm² verwendet. Der relativ hohe Kabelquerschnitt soll für einen geringen Spannungsabfall sorgen. Durch die Verwendung von mehreren Netzteilen ist dieser jedoch nun nicht mehr notwendig. Hier würde sich nun ein geringerer Querschnitt anbieten, auch um eine einfachere Verbindung zu den Raspberry Pi Zero W zu ermöglichen. Diese wurden auf Seiten der Zero W verlötet und in den Verteilerdosen mit Federkraftklemmen verbunden (siehe [img:stromverteilung]).

Die Stromversorgung der Beleuchtung erfolgt über ein 12-V-Netzteil mit 3, 5 A Ausgangsleistung. Auch hier wurde Klingeldraht zur Verteilung zwischen den einzelnen Holmen genutzt.

Beim WLAN-Router war ein entsprechendes USB-Netzteil mitgeliefert.

Kommunikation und Datenübertragung

Die Kommunikation zwischen den Raspberry Pis erfolgt über WLAN. Vorteil dieser Lösung ist, dass hier keine weiteren Leitungen außer der Stromversorgung zu den einzelnen Raspberry Pi Zero W benötigt wird und es auch möglich wäre, die gleiche Hard- und Software auch für ein größeres System ohne Änderungen zu nutzen. Nachteilig ist die Verbindungsgeschwindigkeit, gerade im Hinblick auf die Synchronisierung der Kameras. Diese Problematik soll aber durch entsprechende Programmierung der Software möglichst klein gehalten werden.

Als weitere Datenleitungen wird eine Steuerleitung für die LED-Streifen benötigt. Über diese erfolgt die Steuerung der einzelnen LED-Gruppen. Auch hier wurde wieder Klingeldraht verwendet.

Voruntersuchungen

Vor und während des Aufbaues des eigentlichen Messystemes wurden einige Voruntersuchungen durchgeführt. Diese dienten dazu, die Machbarkeit des Systemes zu prüfen und die notwendigen Schritte zu ermitteln und zu optimieren. Hauptsächlich ging es hierbei um die Ermittlung der Kamerakonstanten und der Verzeichnung der Kamera. Auch wurde die Möglichkeit der Erstellung eines 3D-Modells durch Fokusstacking wurde untersucht. Die einzelnen Untersuchungen werden im Folgenden kurz vorgestellt.

Änderung der Kamerakonstante durch Fokussierung

These

Die Kamerakonstante einer Kamera ändert sich durch die Fokussierung. Bei gleich eingestellter Objektdistanz ist die Kamerakonstante näherungsweise gleich sein.

Ziel

In wird eine Formel (siehe [eq:kraus_fokus]) für die Bildweite b in Abhängigkeit des Gegenstandsweite g gegeben. Die Bildweite entspricht näherungsweise der Kamerakonstante . Die Nutzung der Formel als Näherungswert soll überprüft werden und eine optimierte Formel für die Raspberry Pi Kamera ermittelt werden.

$$\begin{aligned}
    \frac{1}{f} = \frac{1}{g} + \frac{1}{b}
    \label{eq:kraus_fokus}
\end{aligned}$$

Vorgehen

Die Änderungen wurden in einem Versuch beobachtet. Hierzu wurde der Raspberry Pi Zero mit montierter Kamera fest vor einem Charuco-Kalibriermuster platziert. Dieser ermöglicht auch bei unscharfen Bildern noch eine gute automatische Erkennung der zu beobachtenden Punkte. Es wurden je 11 Bilder mit unterschiedlichen Fokussierungen von 2 m bis 10 cm aufgenommen. Dieser Vorgang wurde insgesamt viermal wiederholt, um auch die Wiederholungsgenauigkeit zu ermitteln. Die Bilder wurden anschließend mit einem Python-Script unter Nutzung von OpenCV ausgewertet. Hierbei wurde die relative Veränderung der Kamerakonstante der Kamera ermittelt und mit dem erwarteten Wert verglichen. Die relativen Änderungen wurden auf eine Fokusdistanz von 20 cm (entspricht einer Kamerakonstante von 5 dpt) normiert. Relative Angaben wurden genutzt, da noch keine Kamerakonstante zu dem Zeitpunkt vorlag oder angenommen werden sollte.

Ergebnis

Die Ergebnisse sind in [img:fokus_faktor] dargestellt. Es zeigt sich, dass die Änderungen der Kamerakonstante durch die Fokusierung linear zu der Dioptrienzahl (Kehrwert der Gegenstandsweite) sind. Ein lineares Verhältnis ergibt sich auch mit der im Datenblatt angegebenen Brennweite von 4, 74 mm, jedoch eine etwas flachere Gerade (blau). Die ausgleichende Gerade (blau) ergab eine Brennweite von 6, 97 mm. Die Abweichung von 2, 23 mm entspricht einer Abweichung von 47%, was als unrealistisch hoch eingeschätzt wird. Hier scheinen sich weitere Effekte bemerkbar zu machen, die noch nicht berücksichtigt wurden. Es wurde daher auch die Verzeichnung weiter untersucht.

[]

Box-Whisker-Plot der relativen Veränderung der Kamerakonstante normalisiert auf eine Fokusdistanz von 20 cm (5 dpt)

Änderung der Verzeichnung mittels Charuco-Board

These

Durch die Verschiebung der Linsen beim Fokusieren verändert sich auch die Verzeichnung der Kamera.

Ziel

Die Veränderung der Verzeichnung soll ermittelt und eine Korrekturformel ermittelt werden.

Vorgehen

Der Versuchsaufbau von der Bestimmung der relativen Änderung der Kamerakonstante blieb bestehen. Es wurden jedoch zusätzlich die Verzeichnungen der Bilder ermittelt. Die Verzeichnung wurde mit OpenCV ermittelt und mit der erwarteten Verzeichnung verglichen.

Ergebnis

Die Ergebnisse waren nicht zufriedenstellend. Es zeigte sich, dass die Verzeichnung mit nur einer Aufnahme pro Fokussierung nicht zufriedenstellend modelliert werden konnte.

Bestimmung des Zusammenhang zwischen innerer Orientierung und Fokussierung

These

Die innere Orientierung ist von der Fokussierung abhängig.

Ziel

Es soll eine Formel zur Bestimmung von Näherungswerten für die innere Orientierung in Abhängigkeit der Fokussierung ermittelt werden.

Vorgehen

Es wurden mit fünf verschiedenen Fokussierungen mit jeweils 24 Raspberry-Pi-Kameras Bilder aufgenommen und die Bilder in Agisoft Metashape mittels Aruco-Marker orientiert, dessen Position bekannt war (siehe [sec:kalibrierung]). Außerdem wurden etwa 100 Schneider-Marker im Bildbereich der Kameras verteilt und als Verknüpfungspunkte benutzt. Es wurde jeweils in Metashape alle Bilder mit der gleichen Fokussierung als eine Kamera angenommen und die innere Orientierung bestimmt. Anschließend wurden alle Kameras nochmal einzeln ausgeglichen. Die innere Orientierung wurde in Form von Brennweite, Bildhauptpunktverschiebung und Verzeichnung ermittelt. Die Ergebnisse wurden in einem Box-Whisker-Plot dargestellt und eine ausgleichende Gerade ermittelt.

Ergebnis

Die Ergebnisse sind in [img:naeherungswerte] dargestellt. Wie auch schon in der vorherigen Untersuchung zeigt sich, dass die Brennweite linear zur Fokussierung ist. Bei der Bildhauptpunktverschiebung und der Verzeichnung ist die Abhängigkeit nicht eindeutig. [tab:naeherungswerte_corr] zeigt die Korrelationsmatrix der Näherungswerte. Es zeigt sich, dass die Fokussierung und die Brennweite stark korreliert sind. Die Bildhauptpunktverschiebung ist nur schwach korreliert. Die Verzeichnung ist nicht korreliert. Es zeigt sich, dass die innere Orientierung durch die Fokussierung beeinflusst wird. Die Brennweite ist dabei erwartungsgemäß am stärksten betroffen.

[]

Box-Whisker-Plots und ausgleichende Gerade der inneren Orientierung in Abhängigkeit von der Fokussierung [dpt]

                  Fokus [dpt]       f       cx       cy       k1       k2       k3
  ------------- ------------- ------- -------- -------- -------- -------- --------
  Fokus [dpt]           1,000   0,901    0,032   -0,128   -0,069   -0,181    0,177
  f                             1,000   -0,010   -0,211   -0,220   -0,071    0,092
  cx                                     1,000   -0,230    0,005    0,005    0,005
  cy                                              1,000    0,031    0,023   -0,051
  k1                                                       1,000   -0,925    0,866
  k2                                                                1,000   -0,985
  k3                                                                         1,000

  : Korrelationsmatrix der Näherungswerte

3D-Modell aus Fokusstacking

These

Durch Fokusstacking kann ein besseres 3D-Modell erstellt werden.

Ziel

Es soll geprüft werden, ob durch Fokusstacking die Qualität des 3D-Modell verbessert werden kann. Hierzu wird ein 3D-Modell aus einem Fokusstacking erstellt und mit einem normalen 3D-Modell verglichen.

Vorgehen

-   Automatisierter Fokusstacking

-   keine Beachtung der festen Ausrichtung

-   Transformation über SIRF und Homographie

Software-Entwicklung

Für die Steuerung der Kameras und die anschließende Berechnung des 3D-Modelles muss ein Betriebssystem für das Kamerasystem und eine entsprechende Schnittstelle zu einer SfM-Software geschaffen werden. Diese Entwicklung erfolgte hauptsächlich in Python in Form von Prototyping. Das Kapitel beschreibt die Anforderungen an die Software in [sec:Anforderungsanalyse]. Anschließend werden hieraus die Anwendungsfälle ([sec:Anwendungsfallmodellierung]) erarbeitet und abschließend die Implementation ([sec:Implementierung]) beschrieben.

Anforderungsanalyse

Funktionale Anforderungen

-   Die Kameras sollen zeitgleich auslösbar sein. Die Auslösung soll möglichst verzögerungsfrei erfolgen.

-   Die Steuerung soll auch unabhängig von anderen Geräten möglich sein, beispielsweise per Tastensteuerung.

-   Der Status des Systemes soll für den Nutzer erkennbar sein - auch ohne Anschluss eines Computers etc..

-   Es sollen Passpunkte automatisch gefunden und und für die Bestimmung der äußeren Orientierung genutzt werden.

-   Die Bilder sollen scharf und fokussiert sein.

Schnittstellen

-   Die Daten sollen intern gespeichert werden.

-   Eine Speicherung auf tragbaren Speichermedien wie USB-Sticks soll möglich sein.

-   Eine direkte Übertragung an SfM-Software soll möglich sein.

Nicht-funktionale Anforderungen

-   Die Erfassung soll ohne weitere Hardware möglich sein. Das System soll unabhängig von Netzwerkanschlüssen etc. sein.

-   Alle Kommunikation soll über WLAN erfolgen.

Anwendungsfallmodellierung

Entsprechend der benötigten Schritte aus [c:photogrammetrie] und [img:ablauf] wurde die Anwendungsfälle, die die Benutzeroberfläche ermöglichen soll, im Anwendungsfall-Diagramm in [img:anwendungsfall] zusammengetragen.

[]

Anwendungsfall-Diagramm

Aus den benötigten Daten wurde das Domänen-Klassendiagramm aus [img:dokladia] erzeugt. Dieses zeigt vor allem die Abhängigkeiten der einzelnen Datensätze untereinander.

[]

Domänen-Klassendiagramm

Implementierung

Die Programmierung des Systemes erfolgte iterativ. Einzelne Arbeitspakete wurden in einem Jupyter-Notebook ausprobiert und dann, wenn dieser Schritt erfolgreich war, in den Gesamtworkflow integriert. Größtenteils wurden der Python-Code objektorientiert und typisiert geschrieben.

Module auf den Raspberry Pi’s (Python)

Allgemeine Module und Bibliotheken

Es wurde, wenn möglich, auf fertige Python-Bibliotheken zurückgegriffen. Hierdurch sollte der Programmieraufwand verringert und auf bereits getesteten Code gesetzt werden. Die wichtigsten Bibliotheken sind:

OpenCV

ist eine Bibliothek für Bildbearbeitung und maschinelles Sehen. Sie ist weit verbreitet und bietet viele photogrammetrische Funktionen. Hiermit wurde beispielsweise die Detektion von Markern durchgeführt und die Näherungswerte der Kameras berechnet.

NumPy

bietet neben vielen weiteren Funktionen die Möglichkeit der Matrizenrechnung. Diese wurde für viele Berechnungen benötigt, beispielsweise für die Berechnungen der Kamera-Projektionen.

SciPy

wurde für die Berechnung der Bündelblockausgleichung verwendet. Der manuelle Ansatz mit den Formeln aus unter Nutzung von NumPy war sehr ressourcenlastig. Unter Verwendung von SciPy und der Projektionsgleichung konnte die Berechnungsdauer stark dezimiert werden.

Flask

wurde genutzt um die Weboberfläche und die Datendownloads bereitzustellen. Hiermit wurde ein Webserver aufgesetzt, der die Daten der Kameras anzeigt und die Steuerung ermöglicht.

Außerdem wurden einige allgemeine Klassen erstellt, die in allen Modulen genutzt wurden. Diese sind in [img:uml_common] dargestellt. Diese steuern allgemeine Funktionen wie das Logging und das Auslesen der Konfiguration. Außerdem legen die Interfaces die Struktur der Datenübertragung zwischen den Raspberry Pi’s fest.

[]

Common

Kamera-Steuerung

Die Raspberry Pi Zero W übernehmen die Steuerung der Kameras. Hierfür wurde ein Modul entwickelt, das die Kameras steuert, die Bilder aufnimmt und anschließend zur Verfügung stellt. Die Klassen sind in [img:uml_camera] dargestellt.

[]

Camera

Master-Steuerung

[]

Master

Desktop-Programm (Java)

[]

Screenshot der Connector-Software unter Ubuntu 24.04

[]

Connector

Systemkalibrierung

-   Kameramodellierung

-   Kamerakalibrierung

-   Kameraausrichtung

Genauigkeitsuntersuchungen

-   Kameraanzahl

-   Drehteller

-   Vergleichsmessung

-   

Ausblick und Fazit

Vor allem das Erzeugen der Näherungswerte in Vorbereitung der Bündelblockausgleichung benötigte deutlich mehr Zeit und Theorieverständnis als gedacht. Daher wurde leider nicht alle ursprünglich geplanten Features umgesetzt. Aufgrund von Krankheit und anderen Uni-Projekten konnte dann zusätzlich auch nicht so viel Zeit in der zweiten Semesterhälfte in das Projekt gesteckt werden, wie eigentlich ursprünglich gedacht. Es sind bisher beispielsweise keine Nebenbedingungen möglich - ein Festlegen eines Maßstabes aufgrund einer bekannten Strecke ist so nicht möglich und auch nicht die Optimierung der Ausrichtung durch die Angabe gleich hoher Punkte. Auch wird aktuell nur die Position, nicht jedoch die bereits errechnete Drehung in den EXIF-Daten gespeichert. Hierfür müsste noch eine Umrechnung der Drehung aus dem System der ECEF-Koordinaten in die für EXIF-Daten übliche Ausrichtung an der Lotrichtung der Ortes des Bildes erfolgen. Ein weiteres offenes Problem ist die bereits erwähnte Transformation des lokalen Koordinatensystemes. Hier müsste noch die Ausgleichung so optimiert werden, dass nur ein Maßstab und keine Scherung verwendet wird.

Neben den erwähnten fehlenden Funktionen wäre als weitere Erweiterungen eine Berechnung einer dichten Punktwolke denkbar. Entsprechende Bibliotheken wurden während der Entwicklung entdeckt und schienen relativ leicht einbaubar. So würde die Software zu einer Komplettlösung für SfM-Punktwolken aus Bildern werden.

Im Gesamten sorgte das Projekt dafür, ein tiefergehendes Verständnis von Photogrammmetrie im Allgemeinen und SfM im Speziellen zu erarbeiten sowie vor allem die Probleme und Schwierigkeiten kennenzulernen.

Bedienungsanleitung

Zweck

Ziel des Systemes ist es, 3D-Modelle von Objekten bis zu einer Größe von 40 cm Durchmesser zu erstellen. Die Bedienung soll dabei möglichst einfach und selbsterklärend sein, um auch Laien die Möglichkeit zu geben, das System zu bedienen.

Inbetriebnahme

Beim Aufstellen ist darauf zu achten, dass sich keine stärkeren seitlichen Lichtquellen um das System herum befinden, wie beispielsweise auch Fensterflächen. Diese könnten die Belichtung der Bilder beeinflussen und so die Qualität der 3D-Modelle negativ beeinflussen. Gegebenenfalls muss für Verschattung oder Streuung gesorgt werden.

Die Berechnung des 3D-Modelles erfolgt auf einem externen Rechner. Hier kann wahlweise Agisoft Metashape oder OpenDroneMap (NodeODM) genutzt werden. Die entsprechende Software sowie eine Java-Laufzeitumgebung müssen auf dem Rechner installiert sein und die Bilder müssen auf diesen übertragen werden. Die Übertragung kann automatisch über eine Netzwerkverbindungen oder manuell per USB-Stick erfolgen. Die mitgelieferte Verbindungssoftware muss auf dem Rechner gestartet sein (siehe [sec:SoftwareEinrichtung]).

Das System startet bei Anschluss an eine Stromversorgung selbstständig. Da die Gefahr besteht, dass die kamerasteuernden Raspberry Pi Zero Daten verlieren, wenn die Stromversorgung unterbrochen wird, sollte das System immer ordnungsgemäß heruntergefahren werden und auf eine zuverlässige Stromversorgung geachtet werden. Das Abschalten erfolgt durch langes Drücken auf den roten Taster. Das System fährt dann selbstständig herunter - erkennbar an dem Erlöschen der LEDs der Raspberry Pi Zero und der Beleuchtung - und die Stromversorgung kann getrennt werden.

Software-Einrichtung

Die Software zur Steuerung der Kameras und zur Übertragung der Bilder auf den Rechner ist in Java geschrieben. Sie kann unter Linux, Windows und MacOS genutzt werden. Auf dem Rechner muss entsprechend eine Java-Laufzeitumgebung installiert sein. Für die Nutzung von Metashape muss eine Lizenz vorhanden sein und neben der ausführbaren jar-Datei abgelegt werden. Für OpenDroneMap muss die Software in Form von NodeODM auf dem Rechner installiert sein. Die Verbindung zu einem Server ist nicht implementiert.

Kalibrierung

Die letzten Koordinaten der Passpunkte wird im System gespeichert - daher sollten diese möglichst nicht verändert werden. Falls diese dennoch verändert werden, kann das System einzelne Veränderungen berechnen und nutzen. Bei Änderung einer Vielzahl muss das System jedoch extern neu kalibriert werden, beispielsweise durch Bilder mit einer externen Kamera, wo durch dann die Koordinaten der Passpunkte neu bestimmt werden können.

Eine Kalibrierung mit Bordmitteln ist nicht möglich.

Durchführung

Das System wird gestartet in dem die Stromversorgung hergestellt wird. Die Kameras starten selbstständig und die Beleuchtung wird eingeschaltet. Nach kurzer Zeit sollten kurz alle LEDs grün leuchten. Falls nicht, sind einige Kameras nicht erreichbar. Problemlösungen werden im Kapitel [sec:Problembehandlung] behandelt.

Das Objekt wird mittig, ggf. auf einer Erhöhung im Rahmen positioniert.

Ab hier trennen sich die Wege je nach verwendeten System. Die Bildaufnahme erfolgt in allen Fällen durch kurzen Druck auf den grünen Taster.

[]

Screenshot der Connector-Software unter Ubuntu 24.04

mit Netzwerkverbindung

Der zu verwendende Rechner wird mit dem System per WLAN (bevorzugt) oder Netzwerkkabel verbunden. Die Verbindungssoftware wird gestartet und die IP-Adresse des Systemes angegeben werden. Standardmäßig lautet diese im WLAN 10.0.1.1, per Netzwerkkabel muss die IP von einem DHCP-Server festgelegt werden, beispielsweise in dem der angeschlossene Rechner als DHCP-Server konfiguriert wird. Unter Gnome ist dieses beispielsweise möglich, indem in den Netzwerkeinstellungen die Internetverbindung des PC freigegeben wird.

Anschließend wird die zu nutzende Software ausgewählt und die Verbindung hergestellt.

Nun können die Bilder aufgenommen werden (grüne Taste). Die Bilder werden auf den Rechner übertragen und dort in der ausgewählten Software weiterverarbeitet. Der Fortschritt ist in der rechten Hälfte der Verbindungssoftware zu erkennen.

Durchführung ohne direkte Weiterverarbeitung

Alternativ können die Bilder auch ohne angeschlossenen PC aufgenommen werden und später weiter verarbeitet werden. Die Bilder werden in allen Fällen auf dem Raspberry Pi 4, der die gesamte Steuerung übernimmt, gespeichert. Die Bilder können dann von der Website des Raspberry Pi heruntergeladen werden. Auch ist es möglich, einen USB-Stick in den USB-Port des Raspberry Pi 4 vor der Bildaufnahme zu stecken. Die Bilder werden dann auch auf den USB-Stick kopiert.

Die Verbindungssoftware unterstützt auch das Laden der Bilder aus einem lokalen Ordner, beispielsweise aus der ZIP der Website oder dem Ordner auf dem USB-Stick.

Wartung

Software-Updates können zu Inkompatibilitäten führen, daher sollten diese erstmal auf einem zusätzlichen Raspberry Pi ausprobiert werden, bevor diese auf dem System ausgerollt werden.

Fehlerbehebung

Kameras sind nicht erreichbar

Die Kameras sind nicht erreichbar, wenn die LEDs nicht grün leuchten. Dies kann verschiedene Ursachen haben. Als erstes sollte das System nochmal heruntergefahren werden durch einen langen Druck auf die rote Taste. Anschließend wird die Stromversorgung für einige Sekunden vollständig getrennt und wieder hergestellt. Das System sollte nun neu starten und die Kameras erreichbar sein.

Falls das noch nicht der Fall ist, hilft ein Blick in das Menü des WLAN-Routers. Hier sollten in der Übersicht aller Netzwerkgeräte die 24 Kameras, der steuernde Raspberry Pi 4 und das Gerät, über das der Zugriff erfolgt, aufgezählt sein. Falls das nicht der Fall ist, muss der fehlerhafte Raspberry Pi Zero an einen Display und eine Tastatur angeschlossen werden, um den Fehler zu finden.

Falls der Raspberry Pi im Netzwerk zu finden ist, kann sich per SSH mit dem Raspberry Pi verbunden werden. Zugangsdaten sind die Übersicht am Ende zu entnehmen.

Zugangsdaten

  Gerät               Zugang        Benutzer   Passwort
  ------------------- ------------- ---------- ----------------
  WLAN-Netzwerk       photobox                 photogrammetry
  Raspberry Pi 4      10.0.1.1      photo      box
  Raspberry Pi Zero   10.0.2.1-24   photo      box
  WLAN-Router         10.0.0.1                 photobox1

Teileliste

Mechanische Bauteile

  Bezeichnung                      Anzahl    Einheit   pro Einheit   Gesamtpreis
  ------------------------------- -------- --------- ------------- -------------
  Alu-Strebenprofil Nut 6 Typ B    5, 8 m     0, 1 m     0, 44 EUR    25, 52 EUR
  Eckwürfel                          8             1     5, 00 EUR    40, 00 EUR
  90-Grad-Winkel                     16            1     1, 40 EUR    22, 40 EUR
  45-Grad-Winkel                     16           10    23, 00 EUR    46, 00 EUR
  Hammerkopf-Mutter M4               40            1     1, 40 EUR    56, 00 EUR
  Zylinderschraube M4                40          100     3, 80 EUR     3, 80 EUR
  Scheibe M4                         40          100     1, 85 EUR     1, 85 EUR
  Winkelprofil 30 x 500mm           2 m          2 m    23, 96 EUR    23, 96 EUR
                                                                     219, 53 EUR

  : Mechanische Bauteile mit Preisen (Stand: September 2023)

Elektronische Bauteile

  Bezeichnung                  Anzahl    Einheit   pro Einheit    Gesamtpreis
  --------------------------- -------- --------- ------------- --------------
  Raspberry Pi Zero W            24            1    17, 90 EUR    429, 60 EUR
  Raspberry Pi Camera 3          24            1    29, 15 EUR    699, 61 EUR
  Raspberry Pi 4                 1             1    66, 80 EUR     66, 80 EUR
  RPi Zero Gehäuse + Kabel       24            1     3, 60 EUR      86, 40EUR
  Speicherkarte 32 GB            25            1     5, 95 EUR    142, 80 EUR
  LED-Streifen                   1             1    31, 10 EUR     31, 10 EUR
  Stromversorgung 12V 3,5 A      1             1    11, 70 EUR     11, 70 EUR
  Stromversorgung 5V 7 A         2             1    18, 20 EUR     36, 40 EUR
  Buchse für Netzteile           3             1     2, 30 EUR      6, 90 EUR
  Litze 2*0,75                                10     2, 50 EUR      2, 50 EUR
                                                                 1447, 01 EUR

  : Elektronische Bauteile mit Preisen (Stand: September 2023)

Erklärung
Hiermit versichere ich, dass ich die beiliegende Master-Thesis ohne fremde Hilfe selbstständig verfasst und nur die angegebenen Quellen und Hilfsmittel benutzt habe.
Wörtlich oder dem Sinn nach aus anderen Werken entnommene Stellen sind unter Angabe der Quellen kenntlich gemacht.
Verfasser
Florian Timm
Matrikelnummer: 6028121
Gaiserstraße 2, 21073 Hamburg
E-Mail: florian.timm@hcu-hamburg.de
Erstprüfer
Prof. Dr.-Ing. Thomas Kersten
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: thomas.kersten@hcu-hamburg.de
Zweitprüfer
Dipl.-Ing. Kay Zobel
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: kay.zobel@hcu-hamburg.de
Kurzzusammenfassung
Die Photogrammetrie bietet die Möglichkeit, mit relativ einfacher Technik 3D-Modelle zu erstellen. Die Aufnahme der Bilder ist jedoch sehr zeitaufwendig und daher für die Erfassung vieler Objekte, z.B. bei der Digitalisierung von Museumsobjekten, nicht praktikabel. Systeme mit mehreren fest installierten Kameras setzen in der Regel auf hochwertige Kameras, die jedoch die Hardwarekosten stark in die Höhe treiben.

In dieser Arbeit soll der Lösungsansatz untersucht werden, mehrere kostengünstige Kameras, die fest auf einem Rahmen montiert sind, zu verwenden. Mit Kameras für den Raspberry Pi soll ein photogrammetrisches Messsystem für kleine Objekte aufgebaut werden. Dazu soll eine Schnittstelle zur Synchronisation der Kameras programmiert und eine Möglichkeit zur Kalibrierung der Kameras entwickelt werden. Das Endergebnis soll es im Idealfall auch einem photogrammetrischen Laien ermöglichen, schnell und ohne lange Einarbeitungszeit 3D-Modelle in akzeptabler Auflösung und Qualität zu erzeugen.
Abstract
Photogrammetry offers the possibility of creating 3D models with relatively simple technology. However, capturing the images is very time consuming and therefore not practical for many objects, such as museum digitisation. Systems using multiple fixed cameras usually rely on high quality cameras, which significantly increases hardware costs.

This paper explores the solution of using multiple low-cost cameras mounted on a frame. Cameras for the Raspberry Pi are used to build a photogrammetric measurement system for small objects. This involves programming an interface to synchronise the cameras and developing a way to calibrate the cameras. The end result should ideally enable even a photogrammetric layman to produce 3D models of acceptable resolution and quality quickly and without a long training period.

Einleitung

Konzept

In Museen besteht vielfach der Wunsch, ihre Exponate zu digitalisieren. Entsprechende Handreichungen des Deutschen Museumsbundes legen auch die Digitalisierung als 3D-Modelle nahe, verweisen aber auf Aufwand und Format-Probleme . Auch bei Ausgrabungen aber auch in anderen Bereichen besteht der Bedarf dreidimensionale Modelle einfach und kostengünstig zu erfassen.

Im Rahmen dieser Arbeit soll ein Kamerasystem basiert auf Raspberry-Pi-Kameras entwickelt und untersucht werden, in wie weit es diesen Anforderungen gerecht wird. Es soll mittels Photogrammetrie mit geringen personellen Aufwand kleine Objekte bis etwa 40 cm Durchmesser erfassen. Die Bedienung soll dabei laiensicher und mit nur kurzer Einarbeitungszeit möglich sein, dass System also die meisten Schritte selbstständig durchführen. Auch der Nachbau soll mit etwas handwerklichen Geschick möglich sein. Um Lizenzkosten zu sparen, soll die Möglichkeit auf OpenSource-Software zu nutzen geprüft werden.

Als Prototyp soll ein System mit 24 Kameras gebaut werden. Neben der eigentlichen Entwicklung und Untersuchung dieses Systemes soll abschließend die Anzahl der Kameras und die Nutzung eines Drehtellers evaluiert werden, um hiermit gegebenenfalls die Hardwarekosten weiter zu senken oder die Auflösung und Genauigkeit zu steigern.

Stand der Thematik

Es gab bereits einige Arbeiten zu ähnlichen Theman. Hauptsächlich unterschieden diese sich in der Wahl der Kameras und der Zielgruppe der Bediener. Häufig wurden hochwertige Kameras verwendet, die jedoch hohe Kosten in der Anschaffung verursachen. Beispielsweise wurde schon 1990 ein System von Leica entwickelt,

Photogrammmetrische Grundlagen

Das zu entwickelnde System soll 3D-Modelle von Objekten erstellen. Hierfür wird Photogrammmetrie in Form einer SfM-Pipeline genutzt. Der allgemeine Ablauf ist in [img:ablauf] dargestellt. Zunächst werden die Bilder aufgenommen (siehe [s:bilder]). Hierbei ist es wichtig, dass die Bildinhalte sich überlappen. Die Bilder werden dann verknüpft, indem identische Punkte in den Bildern identifiziert werden (siehe [s:verknuepfung]). Hierfür können beispielsweise ArUco-Marker oder die SIFT-Methode genutzt werden. Aus den identifizierten Punkten werden dann die Positionen und Ausrichtung der Kameras und Verknüpfungspunkte in einem lokalen Koordinatensystem ohne bekannten Maßstab berechnet werden. Die berechneten Daten werden anschließend in einer Bündelblockausgleichung gemeinsam optimiert (siehe [s:buendelblock]). Durch die Nutzung von bekannten Größen beispielsweise durch Maßstäbe kann dieses System transformiert werden.

Dieses Kapitel beschreibt die hierfür notwendigen Bedingungen und die Grundlagen der Rekonstruktion des Objektes als 3D-Modell.

[]

Ablauf der Bildverknüpfung, nach

Bilder

Die Berechnung der Tiefeninformationen ist nur möglich, sofern der Punkt in mindestens einem weiteren Bild abgebildet ist. Die Genauigkeit der Berechnung ist vom Schnittwinkel dieser beiden Strahlen abhängig. Um möglichst gute Schnitte zur Verfügung zu haben und die innere und äußere Orientierung möglichst gut berechnen zu können, müssen diese Bilder einige Bedingungen erfüllen.

Überlappung und Bildinhalte

Da die Bilder durch identische Punkte verbunden werden, müssen die Bildinhalte sich überlappen. Die automatische Identifikation von identischen Punkten ist auf verschiedene Weisen möglich: Entweder durch die Nutzung von codierten Passpunkten wie ArUco-Markern oder konzentrischen Passpunkten nach . Alternativ können auch eine Merkmalsextraktion zur Indentifikation von gemeinsamen Punkten genutzt werden, beispielsweise durch die SIFT-Methode. Hierfür muss die oberfläche aber genügend Texturen aufweisen.

Belichtung

und bei sich stark ändernden Helligkeitsverhältnissen auch hilfreich, jedoch wird hierdurch auch die Helligkeit der Verknüpfungspunkte verändert, was wiederum problematisch sein kann. Entsprechend ist es empfehlenswert bei gleichmäßiger Beleuchtung, die sich auch nicht ändern sollte, die Bilder zu erstellen - also beispielsweise bei bedeckten Himmel.

Position und Ausrichtung der Kamera

Bilder, die vom gleichen Standpunkt aufgenommen wurden, sind oft nur ungenau verknüpfbar. Daher empfiehlt es sich, Bilder aus verschiedensten Richtungen zu machen, also bei der manuellen Photographie um das Objekt herumzugehen - eine Mehrbildaufnahme im Rundum-Verband zu erzeugen. Entsprechend müssen die Kameras an dem Trägern auch gleichmäßig um das Objekt verteilt positioniert werden und dabei auch an die Form des Objektes wie Einschnitte anpassbar sein.

Innere Orientierung

Aus der Position eines Punktes in einem Bild kann vereinfacht gedacht ähnlich einer Messung mit einem Theodolit die Richtung des Punktes in Relation zu der Kamera bestimmt werden. Damit diese Berechnung möglich wird, müssen die Parameter der Kamera bekannt sein, die sogenannte innere Orientierung. Sie beschreibt die Abbildung der Kamera mathematisch. Wichtigste Parameter sind hierbei die Lage des Bildhauptpunktes und die Kamerakonstante. Außerdem zählen hierzu auch die Parameter, die die Verzeichnung beschreiben.

Die innere Orientierung kann während der Messung beispielsweise mittels Bündelblockausgleichung bestimmt werden. Bedingung hierfür ist jedoch, dass die innere Orientierung stabil ist und sich nicht während der Messung ändert .

Jede Einstellung der Kameraoptik verändert die innere Orientierung und auch jede Kamera, auch einer Modellreihe, kann je nach Genauigkeitsanspruch als unterschiedlich angesehen werden. Änderungen können sich beispielsweise durch Umfokussierung oder die Nutzung eines optischen Zoom ergeben, aber auch durch einen mechanisch instabilen Aufbau der Kameras. Daher sollten die Bilder möglichst mit einer Kamera mit festen Einstellungen (Brennweite, Fokus, Blende, Objektiv) aufgenommen werden. Änderungen der Empfindlichkeit (ISO-Zahl) oder Belichtungszeit sind unproblematisch für die innere Orientierung .

Verknüpfungspunkte

Um die einzelnen Bilder verknüpfen zu können, werden identische Punkte zwischen zwei oder mehr Bildern benötigt. Diese können klassisch per Hand erfasst werden, jedoch ist dieses schon bei kleineren Projekten sehr zeitaufwändig. Daher wurde zusätzlich die Möglichkeit genutzt, automatisch Verknüpfungspunkte zu erzeugen.

Codierte Zielmarker

Es gibt verschiedenste Formen von Markern, die automatisch erfasst werden können. Grob unterschieden werden kann in codierte und nicht codierte Zielmarker. Beispiele für nicht codierte sind beispielsweise einfache kreisförmige Klebepunkte oder Marker, die aus dem Linien bestehen und ihren Mittelpunkt durch dessen Schnitt definieren. Vorteilhaft ist jedoch die Verwendung von codierten Zielmarken. Hier können die Punkte ihren Nummern direkt zugeordnet werden.

ArUco-Marker

Eine Variante der automatischen Verknüpfungspunkte sind die sogenannten ArUco-Marker. Diese werden häufig für die Orientierung bei Augmented-Reality-Anwendungen genutzt. OpenCV unterstützt die Erkennung dieser Marker. Sie werden als codierte Messmarken verwendet und können automatisch im Subpixelbereich erkannt werden. Jede Ecke kann hier einzeln identifiziert werden, sodass ein erkannter Marker vier Verknüpfungspunkte liefern kann.

Zielmarken nach Schneider

SIFT

Die SIFT-Methode liefert Verknüpfungspunkte aus Mustern auf den photographierten Oberflächen. Es ist meist nicht notwendig explizit Marker an dem aufzunehmenden Objekt anzubringen, sofern seine Oberfläche nicht strukturlos ist (glatte weiße Wände etc.) oder in Bewegung ist.

Zur Erkennung von Merkmalen setzt das Verfahren auf die Detektion von Kanten. Diese werden in verschiedenen Stufen einer Bildpyramide erkannt und ihre Extrema berechnet. Es werden diese Merkmale weiter ausgedünnt, beispielsweise über den Kontrast. Sofern ein möglicher Marker identifiziert wurde, wird eine Beschreibung erzeugt. Diese erfolgt durch Analyse der Helligkeitsabweichungen zu den Nachbar-Pixeln und wird an der stärksten Abweichung ausgerichtet. Hierdurch wird die Beschreibung dann richtungsunabhängig. Mit diesen kann dann die Übereinstimmung von zwei Markern in zwei Photos bestimmt werden, auch wenn die Bilder zueinander gekippt oder gedreht sind.

Verknüpfung von Bildern

Durch die beschriebenen Verfahren und die hieraus entstandenen Verknüpfungspunkte können die Bilder miteinander verknüpft werden. Da die Kamerapositionen am Rahmen veränderlich sind und die Fixierung auch keine ausreichend genaue Fixierung garantiert, können die bekannten Positionen aus vorherigen Messungen nur als Näherungswerte genutzt werden. Die genaue Bestimmung erfolgt der Position und Ausrichtung - die äußere Orientierung - erfolgt dann Über ein Photogrammmetrische Verfahren. Dieses wird im Folgenden vorgestellt.

Abbildungsgleichung

Die Abbildung eines Punktes auf einem Bild wird durch die Abbildungsgleichung beschrieben. In der Matrizenrechnung ergibt sich dieser aus der Multiplikation mit der Projektionsmatrix P. Diese ergibt sich aus der Kameramatrix K, der Rotation R und dem Projektionszentrum X₀. (siehe [abbildungsgleichung], nach und )

$$\begin{aligned}
    \label{abbildungsgleichung}
    x' & = P \cdot X       \\
    P  & = K \cdot [R|X_0] \\
    P  & =
    \begin{bmatrix}
        c_x & 0   & x'_0 \\
        0   & c_y & y'_0 \\
        0   & 0   & 1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        r_11 & r_21 & r_31 & X_0 \\
        r_12 & r_22 & r_32 & Y_0 \\
        r_13 & r_23 & r_33 & Z_0 \\
    \end{bmatrix}
\end{aligned}$$

Um die Beziehung zwischen zwei Bildern aufzustellen, kann man die Abbildungsgleichung nutzen. Da es hier nur um die Beziehung zwischen zwei Bildern geht, kann die Rotation und Translation des ersten Bildes auf 0 gesetzt werden (R ist dann eine 3x3-Einheitsmatrix und X₀ ein Nullvektor). X₀ des zweiten Bildes wird zur Translation zwischen den beiden Bildern.

Rückwärtsschnitt

Die Koordinaten der ArUco-Marker sind aus einer Kalibrierung und aus vorherigen Messungen bekannt. Aus diesen und den Bildkoordinaten der Marker kann die Position und Ausrichtung der Kamera berechnet werden. Hierfür wird der sogenannte Rückwärtsschnitt genutzt. Die Berechung erfolgt auf Basis der Abbildungsgleichung, die die Position eines Punktes in einem Bild in Beziehung zur Kamera setzt. Für die Berechnung selbst gibt es verschiedene Methoden. Verwendet wurde hier der von OpenCV genutzte Ansatz von in Kombination mit einem RANSAC-Ansatz. Durch die Nutzung des RANSAC-Ansatzes können veränderte Passpunkte identifiziert und als Ausreißer markiert werden.

Vorwärtsschnitt

Die Koordinaten der Passpunkt-Ausreißer aus der Berechung der Kamerapositionen werden anschließend neu berechnet. Hierfür wird der Vorwärtsschnitt genutzt. Auch hier wird OpenCV zur Berechnung genutzt. Die Berechnung der Passpunkte erfolgt für jedes Bildpaar einzeln. Für alle Koordinaten wird dann pro Passpunkt der Z-Score berechnet. Passpunkte, die einen Z-Score von über 2 haben, werden als Ausreißer markiert und nicht weiter betrachtet.

Mit dem Vorwärtsschnitt können auch die Neupunkte, die mittels SIFT oder ähnlichen Bilderkennungsalgorithmen erkannt wurden, berechnet werden. Dadurch kann schon eine dünne Punktwolke erzeugt werden, die dann in der Bündelblockausgleichung weiter optimiert werden kann.

Bündelblockausgleichung

Mittels Bündelblockausgleichung können die grob mit den vorher genannten Verfahren bestimmten Positionen und Drehungen in einer Ausgleichung optimiert werden. Hierzu gehen alle Parameter der Bilder und die Positionen der Passpunkte in die gemeinsame Ausgleichung ein. Grundlage der Ausgleichung ist die in [ss:abbildungsgleichung] beschriebene Abbildungsgleichung. Als Ergebnis erhält man die ausgeglichenen Parameter und Genauigkeitsangaben für diese.

Multi-view Stereo

Die dünne Punktwolke aus dem Vorwärtsschnitt kann durch Multi-view Stereo-Verfahren zu einem 3D-Modell erweitert werden. Hierbei werden jeweils Bildpaare gebildet und die Disparitäten, also die Verschiebung des Objektes in der Abbildung, bestimmt. Diese sind abhängig von der Entfernung des Objektes, bei einem unendlich weit entfernten Objekt tritt keine Disparität auf . Die Disparitäten können dann in Tiefeninformationen umgerechnet werden. Diese können dann wiederum gemittelt und zu einen Tiefenbild zusammengefasst werden.

Mesh-Generierung

Bis zu diesem Schritt besteht das Modell nur aus einzelnen Punkten, die keine Oberfläche ergeben. Um ein 3D-Modell zu erhalten, muss eine Oberfläche generiert werden. Hierfür wird ein Mesh-Generierungsverfahren genutzt. Hierbei wird die Punktwolke in Dreiecke unterteilt. Hierfür gibt es verschiedene Verfahren, die sich in der Art der Unterteilung unterscheiden. OpenDroneMap nutzt beispielsweise die Screened Poisson Surface Reconstruction.

Die Screened Poisson Surface Reconstruction ist ein Verfahren, das auf der Poisson-Gleichung basiert. Diese wird genutzt, um die Oberfläche zu glätten und zu interpolieren. Ein Teil des Ansatzes ist es, dass die Ausrichtung der Punkte berückstichtigt wird. Die Punkte werden hierbei in eine Gitterstruktur überführt und die Oberfläche durch die Lösung der Poisson-Gleichung bestimmt.

Texturierung

Abschließend wird das Modell texturiert. Hierfür werden die Bilder, die zur Erstellung des Modells genutzt wurden, auf das Modell projiziert. Außerdem werden Helligkeits- und Farbunterschiede ausgeglichenen.

Aufbau des Messsystemes

Die Kameras sollten eine hohe geometrische Auflösung und möglichst stabile innere Orientierung aufweisen. Außerdem sollen sie während einer Messkampagne nicht in ihrer Lage zueinander verändert werden, damit die äußere Orientierung größtenteils unverändert bleibt. Daher ist ein stabiler Rahmen notwendig, an welchem die Kameras verdrehsicher angebracht werden können. Kleinere Restfehler in den Orientierungen können mit ausgeglichen werden. Um Ungenauigkeiten durch Bewegungen zu verhindern, müssen die Kameras möglichst zeitgleich auslösen. Daher ist eine gemeinsame Steuerung und Kommunikation zwischen den Kameras notwendig. Außerdem sollen alle Bilder dann auf das Steuerungssystem übertragen werden, hierfür wir eine Form der Datenübertragung benötigt. Damit die Bilder möglichst schattenfrei ausgeleuchtet werden, muss Beleuchtung mit eingeplant werden. Außerdem muss die Stromversorgung der einzelnen Kameras sichergestellt sein.

Aus diesen Anforderungen ergeben sich die einzelnen Abschnitte dieses Kapitels.

Kameras

Als Kameras wurde das Raspberry Pi Camera Module 3 verwendet, welches jeweils von einem Raspberry Pi Zero W gesteuert wird. Im Vergleich zu anderen günstigen Kameras wie Webcams oder der ESP32 CAM haben die Kameras eine hohe geometrische Auflösung von 12 Megapixeln und dennoch mit 1, 4 μm relativ große Pixel , was im subjektiven Eindruck eine sehr gute Bildqualität ergibt.

Nachteil und Vorteil zugleich ist, dass die Kamera über einen Autofokus verfügt, der aber auch elektronisch gesteuert manuell fokussieren kann. Dieser verschlechtert die Stabilität der inneren Orientierung (vgl. [s:innereorientierung]) weiter und wurde daher auch besonders im analysiert. Da die Bilder aber Nahbereich zwischen 20 und 70 cm benötigt werden, ist hier die Schärfentiefe niedrig. Der elektronische Fokus, eine wiederholgenaue und damit mathematisch modellierbare Fokussierung vorausgesetzt, ermöglicht hier, Fokusstacking zu nutzen um den Schärfebereich zu vergrößern.

Weiterer Vorteil der Lösung mit einzelnen Raspberry Pis ist es, dass hierdurch bereits die einzelnen Kameraeinheiten Berechnungen wie das Identifizieren von Passpunkten übernehmen könnten und auch durch die Nutzung von Netzwerkverbindungen für die Steuerung das System skalierbar im Sinne der Anzahl der Kameras aber auch der Größenordnung der Abstände zwischen den Kameras.

Rahmen

[]

Kamera-Winkel

[]

Aluminum-Rahmen

Rahmen und Kameramontage-Winkel

Der Rahmen muss möglichst stabil sein, damit die Kameras sich nicht in ihrer Lage verändern können. Jedoch sollte das System auch weiterhin transportabel - also nicht zu schwer - und veränderbar bleiben, beispielsweise Kameras für Messreihen in ihrer Lage verändert werden. Der Aufbau aus genormten Bauteilen bietet sich an, um hier ggf. den Nachbau einfach ermöglichen zu können. Außerdem sollte der Rahmen auch gegebenenfalls auseinandernehmbar sein, damit er transportiert werden kann.

Als mögliche Materialien kamen Holz, Stahl und Aluminum in Frage. Aufgrund der einfachen Bearbeitung und der genormten Profile, wurde sich für Aluminiumprofile entschieden. Diese gibt es in verschiedenen Ausführungen mit Nuten an den Seitenflächen, so dass eine einfache Montage, aber auch eine Demontage zu Transportzwecken, möglich wird. Außerdem sind diese sehr stabil bei leichtem Gewicht. Durch eine Konstruktion mit Eckwürfeln sowie dem Einbau von dreieckigen Strukturen und Platten, die Scheibenwirkung haben, wurde die Stabilität der Verbindungen erhöht. [img:alurahmen] zeigt den fertigen Rahmen vor Einbau der Platten und der Technik.

Die Kameras wurden mit einem 90^(∘)-Winkel montiert, um diese weiterhin noch vertikal schwenken zu können. [img:aluwinkel] zeigt einen der Winkel, der an den Aluprofilen befestigt ist (noch ohne entsprechende Befestigungsbohrungen).

Beleuchtung

[]

Schattenarme Beleuchtung durch LED-Streifen

[]

Farbige Beleuchtung zur Statusmeldung

Beleuchtung

Um möglichst gute Bilder zu erzeugen, sollte das Objekt gut ausgeleuchtet sein. Eine dunkle Umgebung verlängert die Belichtungszeit, wodurch die Gefahr von unscharfen Aufnahmen steigt. Dunkele Bereiche (ungleichmäßige Ausleuchtung) verursachen verstärktes Rauschen in diesen Bildbereichen. Daher soll das System eine gleichmäßige Ausleuchtung ermöglichen. Problematisch ist hierbei, dass die Kameras ggf. auch die Lichtquellen mit im Bildbereich haben können, wodurch Linsenreflexionen oder Ausbrennen der Bildbereiche möglich sind. Außerdem störend sind fremde Lichtquellen, die Schatten werfen oder die Belichtung der Kameras beeinflussen können.

Es wurde sich zur Beleuchtung für einzeln steuerbare LED-Lichtstreifen entschieden (siehe [img:led_streifen]). Diese können einfach an den Aluprofilen montiert werden und ermöglichen es, einzelne Bereiche und so ggf. blendende Bereiche abzuschalten. Außerdem können hiermit auch verschiedene Lichtfarben eingestellt werden, um Statusmeldungen zu geben (siehe [img:led_farbig]) oder ggf. die Farbgebung des Objektes zu beeinflussen. Die Steuerung erfolgt über einen Raspberry Pi 4, der auch die Steuerung der Kameras übernimmt.

Für diffuseres Licht von außen kann ein halbtransparenter, weißer Stoff über den Rahmen gespannt werden. Dieser sorgt für eine gleichmäßigere Ausleuchtung durch Reflektion im inneren, verhindert auch Reflexionen an Glasscheiben und ähnlichem und vermindert Blendwirkungen durch externe Lichtquellen. Hierfür wurde aus weißem Baumwollstoff eine entsprechende Haube genäht, welche durch zwei mittels Reißverschluss verschließbaren Eingriffmöglichkeiten weiterhin das Einlegen von Objekten durch die Seite oder das Verstellen von Kameras ermöglicht.

Stromversorgung

[]

Elektroverteilung zu den einzelnen Raspberry Pi Zero

Die Raspberry Pis werden mit 5 Volt betrieben. Der Raspberry Pi Zero mit Kamera hatte dabei in Messungen einen maximalen Stromverbrauch von 270 mA aufgezeigt, der Raspberry Pi 4 kann bis zu 1,5 A unter Last verbrauchen. Hieraus ergibt sich ein Gesamtstromverbrauch von maximal rund 8 Ampere. Für den Raspbery Pi 4 wurde ein eigenes Netzteil eingeplant und für die Zero W ein gemeinsames 35 Watt-Netzteil. Versuche zeigten jedoch, dass der Stromverbrauch kurzfristig höher ausfallen konnte, so dass die Zero W, die am meisten von Spannungsabfällen betroffen sind zum Absturz gebracht wurden, wenn alle Kameras gleichzeitig auslösten. Nach dem die Last auf sicherheitshalber auf zwei weitere Netzteile verteilt wurde, lief das System zuverlässig.

Als Kabelmaterial wurde Klingeldraht mit 0, 75 mm² verwendet. Der relativ hohe Kabelquerschnitt soll für einen geringen Spannungsabfall sorgen. Durch die Verwendung von mehreren Netzteilen ist dieser jedoch nun nicht mehr notwendig. Hier würde sich nun ein geringerer Querschnitt anbieten, auch um eine einfachere Verbindung zu den Raspberry Pi Zero W zu ermöglichen. Diese wurden auf Seiten der Zero W verlötet und in den Verteilerdosen mit Federkraftklemmen verbunden (siehe [img:stromverteilung]).

Die Stromversorgung der Beleuchtung erfolgt über ein 12-V-Netzteil mit 3, 5 A Ausgangsleistung. Auch hier wurde Klingeldraht zur Verteilung zwischen den einzelnen Holmen genutzt.

Beim WLAN-Router war ein entsprechendes USB-Netzteil mitgeliefert.

Kommunikation und Datenübertragung

Die Kommunikation zwischen den Raspberry Pis erfolgt über WLAN. Vorteil dieser Lösung ist, dass hier keine weiteren Leitungen außer der Stromversorgung zu den einzelnen Raspberry Pi Zero W benötigt wird und es auch möglich wäre, die gleiche Hard- und Software auch für ein größeres System ohne Änderungen zu nutzen. Nachteilig ist die Verbindungsgeschwindigkeit, gerade im Hinblick auf die Synchronisierung der Kameras. Diese Problematik soll aber durch entsprechende Programmierung der Software möglichst klein gehalten werden.

Als weitere Datenleitungen wird eine Steuerleitung für die LED-Streifen benötigt. Über diese erfolgt die Steuerung der einzelnen LED-Gruppen. Auch hier wurde wieder Klingeldraht verwendet.

Voruntersuchungen

Vor und während des Aufbaues des eigentlichen Messystemes wurden einige Voruntersuchungen durchgeführt. Diese dienten dazu, die Machbarkeit des Systemes zu prüfen und die notwendigen Schritte zu ermitteln und zu optimieren. Hauptsächlich ging es hierbei um die Ermittlung der Kamerakonstanten und der Verzeichnung der Kamera. Auch wurde die Möglichkeit der Erstellung eines 3D-Modells durch Fokusstacking wurde untersucht. Die einzelnen Untersuchungen werden im Folgenden kurz vorgestellt.

Änderung der Kamerakonstante durch Fokussierung

These

Die Kamerakonstante einer Kamera ändert sich durch die Fokussierung. Bei gleich eingestellter Objektdistanz ist die Kamerakonstante näherungsweise gleich sein.

Ziel

In wird eine Formel (siehe [eq:kraus_fokus]) für die Bildweite b in Abhängigkeit des Gegenstandsweite g gegeben. Die Bildweite entspricht näherungsweise der Kamerakonstante . Die Nutzung der Formel als Näherungswert soll überprüft werden und eine optimierte Formel für die Raspberry Pi Kamera ermittelt werden.

$$\begin{aligned}
    \frac{1}{f} = \frac{1}{g} + \frac{1}{b}
    \label{eq:kraus_fokus}
\end{aligned}$$

Vorgehen

Die Änderungen wurden in einem Versuch beobachtet. Hierzu wurde der Raspberry Pi Zero mit montierter Kamera fest vor einem Charuco-Kalibriermuster platziert. Dieser ermöglicht auch bei unscharfen Bildern noch eine gute automatische Erkennung der zu beobachtenden Punkte. Es wurden je 11 Bilder mit unterschiedlichen Fokussierungen von 2 m bis 10 cm aufgenommen. Dieser Vorgang wurde insgesamt viermal wiederholt, um auch die Wiederholungsgenauigkeit zu ermitteln. Die Bilder wurden anschließend mit einem Python-Script unter Nutzung von OpenCV ausgewertet. Hierbei wurde die relative Veränderung der Kamerakonstante der Kamera ermittelt und mit dem erwarteten Wert verglichen. Die relativen Änderungen wurden auf eine Fokusdistanz von 20 cm (entspricht einer Kamerakonstante von 5 dpt) normiert. Relative Angaben wurden genutzt, da noch keine Kamerakonstante zu dem Zeitpunkt vorlag oder angenommen werden sollte.

Ergebnis

Die Ergebnisse sind in [img:fokus_faktor] dargestellt. Es zeigt sich, dass die Änderungen der Kamerakonstante durch die Fokusierung linear zu der Dioptrienzahl (Kehrwert der Gegenstandsweite) sind. Ein lineares Verhältnis ergibt sich auch mit der im Datenblatt angegebenen Brennweite von 4, 74 mm, jedoch eine etwas flachere Gerade (blau). Die ausgleichende Gerade (blau) ergab eine Brennweite von 6, 97 mm. Die Abweichung von 2, 23 mm entspricht einer Abweichung von 47%, was als unrealistisch hoch eingeschätzt wird. Hier scheinen sich weitere Effekte bemerkbar zu machen, die noch nicht berücksichtigt wurden. Es wurde daher auch die Verzeichnung weiter untersucht.

[]

Box-Whisker-Plot der relativen Veränderung der Kamerakonstante normalisiert auf eine Fokusdistanz von 20 cm (5 dpt)

Änderung der Verzeichnung mittels Charuco-Board

These

Durch die Verschiebung der Linsen beim Fokusieren verändert sich auch die Verzeichnung der Kamera.

Ziel

Die Veränderung der Verzeichnung soll ermittelt und eine Korrekturformel ermittelt werden.

Vorgehen

Der Versuchsaufbau von der Bestimmung der relativen Änderung der Kamerakonstante blieb bestehen. Es wurden jedoch zusätzlich die Verzeichnungen der Bilder ermittelt. Die Verzeichnung wurde mit OpenCV ermittelt und mit der erwarteten Verzeichnung verglichen.

Ergebnis

Die Ergebnisse waren nicht zufriedenstellend. Es zeigte sich, dass die Verzeichnung mit nur einer Aufnahme pro Fokussierung nicht zufriedenstellend modelliert werden konnte.

Bestimmung des Zusammenhang zwischen innerer Orientierung und Fokussierung

These

Die innere Orientierung ist von der Fokussierung abhängig.

Ziel

Es soll eine Formel zur Bestimmung von Näherungswerten für die innere Orientierung in Abhängigkeit der Fokussierung ermittelt werden.

Vorgehen

Es wurden mit fünf verschiedenen Fokussierungen mit jeweils 24 Raspberry-Pi-Kameras Bilder aufgenommen und die Bilder in Agisoft Metashape mittels Aruco-Marker orientiert, dessen Position bekannt war (siehe [sec:kalibrierung]). Außerdem wurden etwa 100 Schneider-Marker im Bildbereich der Kameras verteilt und als Verknüpfungspunkte benutzt. Es wurde jeweils in Metashape alle Bilder mit der gleichen Fokussierung als eine Kamera angenommen und die innere Orientierung bestimmt. Anschließend wurden alle Kameras nochmal einzeln ausgeglichen. Die innere Orientierung wurde in Form von Brennweite, Bildhauptpunktverschiebung und Verzeichnung ermittelt. Die Ergebnisse wurden in einem Box-Whisker-Plot dargestellt und eine ausgleichende Gerade ermittelt.

Ergebnis

Die Ergebnisse sind in [img:naeherungswerte] dargestellt. Wie auch schon in der vorherigen Untersuchung zeigt sich, dass die Brennweite linear zur Fokussierung ist. Bei der Bildhauptpunktverschiebung und der Verzeichnung ist die Abhängigkeit nicht eindeutig. [tab:naeherungswerte_corr] zeigt die Korrelationsmatrix der Näherungswerte. Es zeigt sich, dass die Fokussierung und die Brennweite stark korreliert sind. Die Bildhauptpunktverschiebung ist nur schwach korreliert. Die Verzeichnung ist nicht korreliert. Es zeigt sich, dass die innere Orientierung durch die Fokussierung beeinflusst wird. Die Brennweite ist dabei erwartungsgemäß am stärksten betroffen.

[]

Box-Whisker-Plots und ausgleichende Gerade der inneren Orientierung in Abhängigkeit von der Fokussierung [dpt]

                  Fokus [dpt]       f       cx       cy       k1       k2       k3
  ------------- ------------- ------- -------- -------- -------- -------- --------
  Fokus [dpt]           1,000   0,901    0,032   -0,128   -0,069   -0,181    0,177
  f                             1,000   -0,010   -0,211   -0,220   -0,071    0,092
  cx                                     1,000   -0,230    0,005    0,005    0,005
  cy                                              1,000    0,031    0,023   -0,051
  k1                                                       1,000   -0,925    0,866
  k2                                                                1,000   -0,985
  k3                                                                         1,000

  : Korrelationsmatrix der Näherungswerte

3D-Modell aus Fokusstacking

These

Durch Fokusstacking kann ein besseres 3D-Modell erstellt werden.

Ziel

Es soll geprüft werden, ob durch Fokusstacking die Qualität des 3D-Modell verbessert werden kann. Hierzu wird ein 3D-Modell aus einem Fokusstacking erstellt und mit einem normalen 3D-Modell verglichen.

Vorgehen

-   Automatisierter Fokusstacking

-   keine Beachtung der festen Ausrichtung

-   Transformation über SIRF und Homographie

Software-Entwicklung

Für die Steuerung der Kameras und die anschließende Berechnung des 3D-Modelles muss ein Betriebssystem für das Kamerasystem und eine entsprechende Schnittstelle zu einer SfM-Software geschaffen werden. Diese Entwicklung erfolgte hauptsächlich in Python in Form von Prototyping. Das Kapitel beschreibt die Anforderungen an die Software in [sec:Anforderungsanalyse]. Anschließend werden hieraus die Anwendungsfälle ([sec:Anwendungsfallmodellierung]) erarbeitet und abschließend die Implementation ([sec:Implementierung]) beschrieben.

Anforderungsanalyse

Funktionale Anforderungen

-   Die Kameras sollen zeitgleich auslösbar sein. Die Auslösung soll möglichst verzögerungsfrei erfolgen.

-   Die Steuerung soll auch unabhängig von anderen Geräten möglich sein, beispielsweise per Tastensteuerung.

-   Der Status des Systemes soll für den Nutzer erkennbar sein - auch ohne Anschluss eines Computers etc..

-   Es sollen Passpunkte automatisch gefunden und und für die Bestimmung der äußeren Orientierung genutzt werden.

-   Die Bilder sollen scharf und fokussiert sein.

Schnittstellen

-   Die Daten sollen intern gespeichert werden.

-   Eine Speicherung auf tragbaren Speichermedien wie USB-Sticks soll möglich sein.

-   Eine direkte Übertragung an SfM-Software soll möglich sein.

Nicht-funktionale Anforderungen

-   Die Erfassung soll ohne weitere Hardware möglich sein. Das System soll unabhängig von Netzwerkanschlüssen etc. sein.

-   Alle Kommunikation soll über WLAN erfolgen.

Anwendungsfallmodellierung

Entsprechend der benötigten Schritte aus [c:photogrammetrie] und [img:ablauf] wurde die Anwendungsfälle, die die Benutzeroberfläche ermöglichen soll, im Anwendungsfall-Diagramm in [img:anwendungsfall] zusammengetragen.

[]

Anwendungsfall-Diagramm

Aus den benötigten Daten wurde das Domänen-Klassendiagramm aus [img:dokladia] erzeugt. Dieses zeigt vor allem die Abhängigkeiten der einzelnen Datensätze untereinander.

[]

Domänen-Klassendiagramm

Implementierung

Die Programmierung des Systemes erfolgte iterativ. Einzelne Arbeitspakete wurden in einem Jupyter-Notebook ausprobiert und dann, wenn dieser Schritt erfolgreich war, in den Gesamtworkflow integriert. Größtenteils wurden der Python-Code objektorientiert und typisiert geschrieben.

Module auf den Raspberry Pi’s (Python)

Allgemeine Module und Bibliotheken

Es wurde, wenn möglich, auf fertige Python-Bibliotheken zurückgegriffen. Hierdurch sollte der Programmieraufwand verringert und auf bereits getesteten Code gesetzt werden. Die wichtigsten Bibliotheken sind:

OpenCV

ist eine Bibliothek für Bildbearbeitung und maschinelles Sehen. Sie ist weit verbreitet und bietet viele photogrammetrische Funktionen. Hiermit wurde beispielsweise die Detektion von Markern durchgeführt und die Näherungswerte der Kameras berechnet.

NumPy

bietet neben vielen weiteren Funktionen die Möglichkeit der Matrizenrechnung. Diese wurde für viele Berechnungen benötigt, beispielsweise für die Berechnungen der Kamera-Projektionen.

SciPy

wurde für die Berechnung der Bündelblockausgleichung verwendet. Der manuelle Ansatz mit den Formeln aus unter Nutzung von NumPy war sehr ressourcenlastig. Unter Verwendung von SciPy und der Projektionsgleichung konnte die Berechnungsdauer stark dezimiert werden.

Flask

wurde genutzt um die Weboberfläche und die Datendownloads bereitzustellen. Hiermit wurde ein Webserver aufgesetzt, der die Daten der Kameras anzeigt und die Steuerung ermöglicht.

Außerdem wurden einige allgemeine Klassen erstellt, die in allen Modulen genutzt wurden. Diese sind in [img:uml_common] dargestellt. Diese steuern allgemeine Funktionen wie das Logging und das Auslesen der Konfiguration. Außerdem legen die Interfaces die Struktur der Datenübertragung zwischen den Raspberry Pi’s fest.

[]

Common

Kamera-Steuerung

Die Raspberry Pi Zero W übernehmen die Steuerung der Kameras. Hierfür wurde ein Modul entwickelt, das die Kameras steuert, die Bilder aufnimmt und anschließend zur Verfügung stellt. Die Klassen sind in [img:uml_camera] dargestellt.

[]

Camera

Master-Steuerung

[]

Master

Desktop-Programm (Java)

[]

Screenshot der Connector-Software unter Ubuntu 24.04

[]

Connector

Systemkalibrierung

-   Kameramodellierung

-   Kamerakalibrierung

-   Kameraausrichtung

Genauigkeitsuntersuchungen

-   Kameraanzahl

-   Drehteller

-   Vergleichsmessung

-   

Ausblick und Fazit

Vor allem das Erzeugen der Näherungswerte in Vorbereitung der Bündelblockausgleichung benötigte deutlich mehr Zeit und Theorieverständnis als gedacht. Daher wurde leider nicht alle ursprünglich geplanten Features umgesetzt. Aufgrund von Krankheit und anderen Uni-Projekten konnte dann zusätzlich auch nicht so viel Zeit in der zweiten Semesterhälfte in das Projekt gesteckt werden, wie eigentlich ursprünglich gedacht. Es sind bisher beispielsweise keine Nebenbedingungen möglich - ein Festlegen eines Maßstabes aufgrund einer bekannten Strecke ist so nicht möglich und auch nicht die Optimierung der Ausrichtung durch die Angabe gleich hoher Punkte. Auch wird aktuell nur die Position, nicht jedoch die bereits errechnete Drehung in den EXIF-Daten gespeichert. Hierfür müsste noch eine Umrechnung der Drehung aus dem System der ECEF-Koordinaten in die für EXIF-Daten übliche Ausrichtung an der Lotrichtung der Ortes des Bildes erfolgen. Ein weiteres offenes Problem ist die bereits erwähnte Transformation des lokalen Koordinatensystemes. Hier müsste noch die Ausgleichung so optimiert werden, dass nur ein Maßstab und keine Scherung verwendet wird.

Neben den erwähnten fehlenden Funktionen wäre als weitere Erweiterungen eine Berechnung einer dichten Punktwolke denkbar. Entsprechende Bibliotheken wurden während der Entwicklung entdeckt und schienen relativ leicht einbaubar. So würde die Software zu einer Komplettlösung für SfM-Punktwolken aus Bildern werden.

Im Gesamten sorgte das Projekt dafür, ein tiefergehendes Verständnis von Photogrammmetrie im Allgemeinen und SfM im Speziellen zu erarbeiten sowie vor allem die Probleme und Schwierigkeiten kennenzulernen.

Bedienungsanleitung

Zweck

Ziel des Systemes ist es, 3D-Modelle von Objekten bis zu einer Größe von 40 cm Durchmesser zu erstellen. Die Bedienung soll dabei möglichst einfach und selbsterklärend sein, um auch Laien die Möglichkeit zu geben, das System zu bedienen.

Inbetriebnahme

Beim Aufstellen ist darauf zu achten, dass sich keine stärkeren seitlichen Lichtquellen um das System herum befinden, wie beispielsweise auch Fensterflächen. Diese könnten die Belichtung der Bilder beeinflussen und so die Qualität der 3D-Modelle negativ beeinflussen. Gegebenenfalls muss für Verschattung oder Streuung gesorgt werden.

Die Berechnung des 3D-Modelles erfolgt auf einem externen Rechner. Hier kann wahlweise Agisoft Metashape oder OpenDroneMap (NodeODM) genutzt werden. Die entsprechende Software sowie eine Java-Laufzeitumgebung müssen auf dem Rechner installiert sein und die Bilder müssen auf diesen übertragen werden. Die Übertragung kann automatisch über eine Netzwerkverbindungen oder manuell per USB-Stick erfolgen. Die mitgelieferte Verbindungssoftware muss auf dem Rechner gestartet sein (siehe [sec:SoftwareEinrichtung]).

Das System startet bei Anschluss an eine Stromversorgung selbstständig. Da die Gefahr besteht, dass die kamerasteuernden Raspberry Pi Zero Daten verlieren, wenn die Stromversorgung unterbrochen wird, sollte das System immer ordnungsgemäß heruntergefahren werden und auf eine zuverlässige Stromversorgung geachtet werden. Das Abschalten erfolgt durch langes Drücken auf den roten Taster. Das System fährt dann selbstständig herunter - erkennbar an dem Erlöschen der LEDs der Raspberry Pi Zero und der Beleuchtung - und die Stromversorgung kann getrennt werden.

Software-Einrichtung

Die Software zur Steuerung der Kameras und zur Übertragung der Bilder auf den Rechner ist in Java geschrieben. Sie kann unter Linux, Windows und MacOS genutzt werden. Auf dem Rechner muss entsprechend eine Java-Laufzeitumgebung installiert sein. Für die Nutzung von Metashape muss eine Lizenz vorhanden sein und neben der ausführbaren jar-Datei abgelegt werden. Für OpenDroneMap muss die Software in Form von NodeODM auf dem Rechner installiert sein. Die Verbindung zu einem Server ist nicht implementiert.

Kalibrierung

Die letzten Koordinaten der Passpunkte wird im System gespeichert - daher sollten diese möglichst nicht verändert werden. Falls diese dennoch verändert werden, kann das System einzelne Veränderungen berechnen und nutzen. Bei Änderung einer Vielzahl muss das System jedoch extern neu kalibriert werden, beispielsweise durch Bilder mit einer externen Kamera, wo durch dann die Koordinaten der Passpunkte neu bestimmt werden können.

Eine Kalibrierung mit Bordmitteln ist nicht möglich.

Durchführung

Das System wird gestartet in dem die Stromversorgung hergestellt wird. Die Kameras starten selbstständig und die Beleuchtung wird eingeschaltet. Nach kurzer Zeit sollten kurz alle LEDs grün leuchten. Falls nicht, sind einige Kameras nicht erreichbar. Problemlösungen werden im Kapitel [sec:Problembehandlung] behandelt.

Das Objekt wird mittig, ggf. auf einer Erhöhung im Rahmen positioniert.

Ab hier trennen sich die Wege je nach verwendeten System. Die Bildaufnahme erfolgt in allen Fällen durch kurzen Druck auf den grünen Taster.

[]

Screenshot der Connector-Software unter Ubuntu 24.04

mit Netzwerkverbindung

Der zu verwendende Rechner wird mit dem System per WLAN (bevorzugt) oder Netzwerkkabel verbunden. Die Verbindungssoftware wird gestartet und die IP-Adresse des Systemes angegeben werden. Standardmäßig lautet diese im WLAN 10.0.1.1, per Netzwerkkabel muss die IP von einem DHCP-Server festgelegt werden, beispielsweise in dem der angeschlossene Rechner als DHCP-Server konfiguriert wird. Unter Gnome ist dieses beispielsweise möglich, indem in den Netzwerkeinstellungen die Internetverbindung des PC freigegeben wird.

Anschließend wird die zu nutzende Software ausgewählt und die Verbindung hergestellt.

Nun können die Bilder aufgenommen werden (grüne Taste). Die Bilder werden auf den Rechner übertragen und dort in der ausgewählten Software weiterverarbeitet. Der Fortschritt ist in der rechten Hälfte der Verbindungssoftware zu erkennen.

Durchführung ohne direkte Weiterverarbeitung

Alternativ können die Bilder auch ohne angeschlossenen PC aufgenommen werden und später weiter verarbeitet werden. Die Bilder werden in allen Fällen auf dem Raspberry Pi 4, der die gesamte Steuerung übernimmt, gespeichert. Die Bilder können dann von der Website des Raspberry Pi heruntergeladen werden. Auch ist es möglich, einen USB-Stick in den USB-Port des Raspberry Pi 4 vor der Bildaufnahme zu stecken. Die Bilder werden dann auch auf den USB-Stick kopiert.

Die Verbindungssoftware unterstützt auch das Laden der Bilder aus einem lokalen Ordner, beispielsweise aus der ZIP der Website oder dem Ordner auf dem USB-Stick.

Wartung

Software-Updates können zu Inkompatibilitäten führen, daher sollten diese erstmal auf einem zusätzlichen Raspberry Pi ausprobiert werden, bevor diese auf dem System ausgerollt werden.

Fehlerbehebung

Kameras sind nicht erreichbar

Die Kameras sind nicht erreichbar, wenn die LEDs nicht grün leuchten. Dies kann verschiedene Ursachen haben. Als erstes sollte das System nochmal heruntergefahren werden durch einen langen Druck auf die rote Taste. Anschließend wird die Stromversorgung für einige Sekunden vollständig getrennt und wieder hergestellt. Das System sollte nun neu starten und die Kameras erreichbar sein.

Falls das noch nicht der Fall ist, hilft ein Blick in das Menü des WLAN-Routers. Hier sollten in der Übersicht aller Netzwerkgeräte die 24 Kameras, der steuernde Raspberry Pi 4 und das Gerät, über das der Zugriff erfolgt, aufgezählt sein. Falls das nicht der Fall ist, muss der fehlerhafte Raspberry Pi Zero an einen Display und eine Tastatur angeschlossen werden, um den Fehler zu finden.

Falls der Raspberry Pi im Netzwerk zu finden ist, kann sich per SSH mit dem Raspberry Pi verbunden werden. Zugangsdaten sind die Übersicht am Ende zu entnehmen.

Zugangsdaten

  Gerät               Zugang        Benutzer   Passwort
  ------------------- ------------- ---------- ----------------
  WLAN-Netzwerk       photobox                 photogrammetry
  Raspberry Pi 4      10.0.1.1      photo      box
  Raspberry Pi Zero   10.0.2.1-24   photo      box
  WLAN-Router         10.0.0.1                 photobox1

Teileliste

Mechanische Bauteile

  Bezeichnung                      Anzahl    Einheit   pro Einheit   Gesamtpreis
  ------------------------------- -------- --------- ------------- -------------
  Alu-Strebenprofil Nut 6 Typ B    5, 8 m     0, 1 m     0, 44 EUR    25, 52 EUR
  Eckwürfel                          8             1     5, 00 EUR    40, 00 EUR
  90-Grad-Winkel                     16            1     1, 40 EUR    22, 40 EUR
  45-Grad-Winkel                     16           10    23, 00 EUR    46, 00 EUR
  Hammerkopf-Mutter M4               40            1     1, 40 EUR    56, 00 EUR
  Zylinderschraube M4                40          100     3, 80 EUR     3, 80 EUR
  Scheibe M4                         40          100     1, 85 EUR     1, 85 EUR
  Winkelprofil 30 x 500mm           2 m          2 m    23, 96 EUR    23, 96 EUR
                                                                     219, 53 EUR

  : Mechanische Bauteile mit Preisen (Stand: September 2023)

Elektronische Bauteile

  Bezeichnung                  Anzahl    Einheit   pro Einheit    Gesamtpreis
  --------------------------- -------- --------- ------------- --------------
  Raspberry Pi Zero W            24            1    17, 90 EUR    429, 60 EUR
  Raspberry Pi Camera 3          24            1    29, 15 EUR    699, 61 EUR
  Raspberry Pi 4                 1             1    66, 80 EUR     66, 80 EUR
  RPi Zero Gehäuse + Kabel       24            1     3, 60 EUR      86, 40EUR
  Speicherkarte 32 GB            25            1     5, 95 EUR    142, 80 EUR
  LED-Streifen                   1             1    31, 10 EUR     31, 10 EUR
  Stromversorgung 12V 3,5 A      1             1    11, 70 EUR     11, 70 EUR
  Stromversorgung 5V 7 A         2             1    18, 20 EUR     36, 40 EUR
  Buchse für Netzteile           3             1     2, 30 EUR      6, 90 EUR
  Litze 2*0,75                                10     2, 50 EUR      2, 50 EUR
                                                                 1447, 01 EUR

  : Elektronische Bauteile mit Preisen (Stand: September 2023)

Erklärung
Hiermit versichere ich, dass ich die beiliegende Master-Thesis ohne fremde Hilfe selbstständig verfasst und nur die angegebenen Quellen und Hilfsmittel benutzt habe.
Wörtlich oder dem Sinn nach aus anderen Werken entnommene Stellen sind unter Angabe der Quellen kenntlich gemacht.
Aufbau eines photogrammetrischen Messsystems unter Verwendung von Raspberry-Pi-Kameras als Low-Cost-Sensoren
Florian Timm
August 2024

-   Einleitung
    -   Konzept
    -   Stand der Thematik
-   Photogrammmetrische Grundlagen
    -   Bilder
        -   Überlappung und Bildinhalte
        -   Belichtung
        -   Position und Ausrichtung der Kamera
        -   Innere Orientierung
    -   Verknüpfungspunkte
        -   Codierte Zielmarker
        -   SIFT
    -   Verknüpfung von Bildern
        -   Rückwärtsschnitt
        -   Vorwärtsschnitt
    -   Bündelblockausgleichung
    -   Multi-view Stereo
    -   Mesh-Generierung
    -   Texturierung
-   Aufbau des Messsystemes
    -   Kameras
    -   Rahmen
    -   Beleuchtung
    -   Stromversorgung
    -   Kommunikation und Datenübertragung
-   Voruntersuchungen
    -   Änderung der Kamerakonstante durch Fokussierung
    -   Änderung der Verzeichnung mittels Charuco-Board
    -   Bestimmung des Zusammenhang zwischen innerer Orientierung und Fokussierung
    -   3D-Modell aus Fokusstacking
-   Software-Entwicklung
    -   Anforderungsanalyse
        -   Funktionale Anforderungen
        -   Schnittstellen
        -   Nicht-funktionale Anforderungen
    -   Anwendungsfallmodellierung
    -   Implementierung
        -   Module auf den Raspberry Pi’s (Python)
        -   Allgemeine Module und Bibliotheken
-   Systemkalibrierung
-   Genauigkeitsuntersuchungen
-   Ausblick und Fazit

Verfasser
Florian Timm
Matrikelnummer: 6028121
Gaiserstraße 2, 21073 Hamburg
E-Mail: florian.timm@hcu-hamburg.de
Erstprüfer
Prof. Dr.-Ing. Thomas Kersten
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: thomas.kersten@hcu-hamburg.de
Zweitprüfer
Dipl.-Ing. Kay Zobel
HafenCity Universität Hamburg
Überseeallee 16, 20457 Hamburg
E-Mail: kay.zobel@hcu-hamburg.de
Kurzzusammenfassung
Die Photogrammetrie bietet die Möglichkeit, mit relativ einfacher Technik 3D-Modelle zu erstellen. Die Aufnahme der Bilder ist jedoch sehr zeitaufwendig und daher für die Erfassung vieler Objekte, z.B. bei der Digitalisierung von Museumsobjekten, nicht praktikabel. Systeme mit mehreren fest installierten Kameras setzen in der Regel auf hochwertige Kameras, die jedoch die Hardwarekosten stark in die Höhe treiben.

In dieser Arbeit soll der Lösungsansatz untersucht werden, mehrere kostengünstige Kameras, die fest auf einem Rahmen montiert sind, zu verwenden. Mit Kameras für den Raspberry Pi soll ein photogrammetrisches Messsystem für kleine Objekte aufgebaut werden. Dazu soll eine Schnittstelle zur Synchronisation der Kameras programmiert und eine Möglichkeit zur Kalibrierung der Kameras entwickelt werden. Das Endergebnis soll es im Idealfall auch einem photogrammetrischen Laien ermöglichen, schnell und ohne lange Einarbeitungszeit 3D-Modelle in akzeptabler Auflösung und Qualität zu erzeugen.
Abstract
Photogrammetry offers the possibility of creating 3D models with relatively simple technology. However, capturing the images is very time consuming and therefore not practical for many objects, such as museum digitisation. Systems using multiple fixed cameras usually rely on high quality cameras, which significantly increases hardware costs.

This paper explores the solution of using multiple low-cost cameras mounted on a frame. Cameras for the Raspberry Pi are used to build a photogrammetric measurement system for small objects. This involves programming an interface to synchronise the cameras and developing a way to calibrate the cameras. The end result should ideally enable even a photogrammetric layman to produce 3D models of acceptable resolution and quality quickly and without a long training period.

Einleitung

Konzept

In Museen besteht vielfach der Wunsch, ihre Exponate zu digitalisieren. Entsprechende Handreichungen des Deutschen Museumsbundes legen auch die Digitalisierung als 3D-Modelle nahe, verweisen aber auf Aufwand und Format-Probleme . Auch bei Ausgrabungen aber auch in anderen Bereichen besteht der Bedarf dreidimensionale Modelle einfach und kostengünstig zu erfassen.

Im Rahmen dieser Arbeit soll ein Kamerasystem basiert auf Raspberry-Pi-Kameras entwickelt und untersucht werden, in wie weit es diesen Anforderungen gerecht wird. Es soll mittels Photogrammetrie mit geringen personellen Aufwand kleine Objekte bis etwa 40 cm Durchmesser erfassen. Die Bedienung soll dabei laiensicher und mit nur kurzer Einarbeitungszeit möglich sein, dass System also die meisten Schritte selbstständig durchführen. Auch der Nachbau soll mit etwas handwerklichen Geschick möglich sein. Um Lizenzkosten zu sparen, soll die Möglichkeit auf OpenSource-Software zu nutzen geprüft werden.

Als Prototyp soll ein System mit 24 Kameras gebaut werden. Neben der eigentlichen Entwicklung und Untersuchung dieses Systemes soll abschließend die Anzahl der Kameras und die Nutzung eines Drehtellers evaluiert werden, um hiermit gegebenenfalls die Hardwarekosten weiter zu senken oder die Auflösung und Genauigkeit zu steigern.

Stand der Thematik

Es gab bereits einige Arbeiten zu ähnlichen Theman. Hauptsächlich unterschieden diese sich in der Wahl der Kameras und der Zielgruppe der Bediener. Häufig wurden hochwertige Kameras verwendet, die jedoch hohe Kosten in der Anschaffung verursachen. Beispielsweise wurde schon 1990 ein System von Leica entwickelt,

Photogrammmetrische Grundlagen

Das zu entwickelnde System soll 3D-Modelle von Objekten erstellen. Hierfür wird Photogrammmetrie in Form einer SfM-Pipeline genutzt. Der allgemeine Ablauf ist in [img:ablauf] dargestellt. Zunächst werden die Bilder aufgenommen (siehe [s:bilder]). Hierbei ist es wichtig, dass die Bildinhalte sich überlappen. Die Bilder werden dann verknüpft, indem identische Punkte in den Bildern identifiziert werden (siehe [s:verknuepfung]). Hierfür können beispielsweise ArUco-Marker oder die SIFT-Methode genutzt werden. Aus den identifizierten Punkten werden dann die Positionen und Ausrichtung der Kameras und Verknüpfungspunkte in einem lokalen Koordinatensystem ohne bekannten Maßstab berechnet werden. Die berechneten Daten werden anschließend in einer Bündelblockausgleichung gemeinsam optimiert (siehe [s:buendelblock]). Durch die Nutzung von bekannten Größen beispielsweise durch Maßstäbe kann dieses System transformiert werden.

Dieses Kapitel beschreibt die hierfür notwendigen Bedingungen und die Grundlagen der Rekonstruktion des Objektes als 3D-Modell.

[]

Ablauf der Bildverknüpfung, nach

Bilder

Die Berechnung der Tiefeninformationen ist nur möglich, sofern der Punkt in mindestens einem weiteren Bild abgebildet ist. Die Genauigkeit der Berechnung ist vom Schnittwinkel dieser beiden Strahlen abhängig. Um möglichst gute Schnitte zur Verfügung zu haben und die innere und äußere Orientierung möglichst gut berechnen zu können, müssen diese Bilder einige Bedingungen erfüllen.

Überlappung und Bildinhalte

Da die Bilder durch identische Punkte verbunden werden, müssen die Bildinhalte sich überlappen. Die automatische Identifikation von identischen Punkten ist auf verschiedene Weisen möglich: Entweder durch die Nutzung von codierten Passpunkten wie ArUco-Markern oder konzentrischen Passpunkten nach . Alternativ können auch eine Merkmalsextraktion zur Indentifikation von gemeinsamen Punkten genutzt werden, beispielsweise durch die SIFT-Methode. Hierfür muss die oberfläche aber genügend Texturen aufweisen.

Belichtung

und bei sich stark ändernden Helligkeitsverhältnissen auch hilfreich, jedoch wird hierdurch auch die Helligkeit der Verknüpfungspunkte verändert, was wiederum problematisch sein kann. Entsprechend ist es empfehlenswert bei gleichmäßiger Beleuchtung, die sich auch nicht ändern sollte, die Bilder zu erstellen - also beispielsweise bei bedeckten Himmel.

Position und Ausrichtung der Kamera

Bilder, die vom gleichen Standpunkt aufgenommen wurden, sind oft nur ungenau verknüpfbar. Daher empfiehlt es sich, Bilder aus verschiedensten Richtungen zu machen, also bei der manuellen Photographie um das Objekt herumzugehen - eine Mehrbildaufnahme im Rundum-Verband zu erzeugen. Entsprechend müssen die Kameras an dem Trägern auch gleichmäßig um das Objekt verteilt positioniert werden und dabei auch an die Form des Objektes wie Einschnitte anpassbar sein.

Innere Orientierung

Aus der Position eines Punktes in einem Bild kann vereinfacht gedacht ähnlich einer Messung mit einem Theodolit die Richtung des Punktes in Relation zu der Kamera bestimmt werden. Damit diese Berechnung möglich wird, müssen die Parameter der Kamera bekannt sein, die sogenannte innere Orientierung. Sie beschreibt die Abbildung der Kamera mathematisch. Wichtigste Parameter sind hierbei die Lage des Bildhauptpunktes und die Kamerakonstante. Außerdem zählen hierzu auch die Parameter, die die Verzeichnung beschreiben.

Die innere Orientierung kann während der Messung beispielsweise mittels Bündelblockausgleichung bestimmt werden. Bedingung hierfür ist jedoch, dass die innere Orientierung stabil ist und sich nicht während der Messung ändert .

Jede Einstellung der Kameraoptik verändert die innere Orientierung und auch jede Kamera, auch einer Modellreihe, kann je nach Genauigkeitsanspruch als unterschiedlich angesehen werden. Änderungen können sich beispielsweise durch Umfokussierung oder die Nutzung eines optischen Zoom ergeben, aber auch durch einen mechanisch instabilen Aufbau der Kameras. Daher sollten die Bilder möglichst mit einer Kamera mit festen Einstellungen (Brennweite, Fokus, Blende, Objektiv) aufgenommen werden. Änderungen der Empfindlichkeit (ISO-Zahl) oder Belichtungszeit sind unproblematisch für die innere Orientierung .

Verknüpfungspunkte

Um die einzelnen Bilder verknüpfen zu können, werden identische Punkte zwischen zwei oder mehr Bildern benötigt. Diese können klassisch per Hand erfasst werden, jedoch ist dieses schon bei kleineren Projekten sehr zeitaufwändig. Daher wurde zusätzlich die Möglichkeit genutzt, automatisch Verknüpfungspunkte zu erzeugen.

Codierte Zielmarker

Es gibt verschiedenste Formen von Markern, die automatisch erfasst werden können. Grob unterschieden werden kann in codierte und nicht codierte Zielmarker. Beispiele für nicht codierte sind beispielsweise einfache kreisförmige Klebepunkte oder Marker, die aus dem Linien bestehen und ihren Mittelpunkt durch dessen Schnitt definieren. Vorteilhaft ist jedoch die Verwendung von codierten Zielmarken. Hier können die Punkte ihren Nummern direkt zugeordnet werden.

ArUco-Marker

Eine Variante der automatischen Verknüpfungspunkte sind die sogenannten ArUco-Marker. Diese werden häufig für die Orientierung bei Augmented-Reality-Anwendungen genutzt. OpenCV unterstützt die Erkennung dieser Marker. Sie werden als codierte Messmarken verwendet und können automatisch im Subpixelbereich erkannt werden. Jede Ecke kann hier einzeln identifiziert werden, sodass ein erkannter Marker vier Verknüpfungspunkte liefern kann.

Zielmarken nach Schneider

SIFT

Die SIFT-Methode liefert Verknüpfungspunkte aus Mustern auf den photographierten Oberflächen. Es ist meist nicht notwendig explizit Marker an dem aufzunehmenden Objekt anzubringen, sofern seine Oberfläche nicht strukturlos ist (glatte weiße Wände etc.) oder in Bewegung ist.

Zur Erkennung von Merkmalen setzt das Verfahren auf die Detektion von Kanten. Diese werden in verschiedenen Stufen einer Bildpyramide erkannt und ihre Extrema berechnet. Es werden diese Merkmale weiter ausgedünnt, beispielsweise über den Kontrast. Sofern ein möglicher Marker identifiziert wurde, wird eine Beschreibung erzeugt. Diese erfolgt durch Analyse der Helligkeitsabweichungen zu den Nachbar-Pixeln und wird an der stärksten Abweichung ausgerichtet. Hierdurch wird die Beschreibung dann richtungsunabhängig. Mit diesen kann dann die Übereinstimmung von zwei Markern in zwei Photos bestimmt werden, auch wenn die Bilder zueinander gekippt oder gedreht sind.

Verknüpfung von Bildern

Durch die beschriebenen Verfahren und die hieraus entstandenen Verknüpfungspunkte können die Bilder miteinander verknüpft werden. Da die Kamerapositionen am Rahmen veränderlich sind und die Fixierung auch keine ausreichend genaue Fixierung garantiert, können die bekannten Positionen aus vorherigen Messungen nur als Näherungswerte genutzt werden. Die genaue Bestimmung erfolgt der Position und Ausrichtung - die äußere Orientierung - erfolgt dann Über ein Photogrammmetrische Verfahren. Dieses wird im Folgenden vorgestellt.

Abbildungsgleichung

Die Abbildung eines Punktes auf einem Bild wird durch die Abbildungsgleichung beschrieben. In der Matrizenrechnung ergibt sich dieser aus der Multiplikation mit der Projektionsmatrix P. Diese ergibt sich aus der Kameramatrix K, der Rotation R und dem Projektionszentrum X₀. (siehe [abbildungsgleichung], nach und )

$$\begin{aligned}
    \label{abbildungsgleichung}
    x' & = P \cdot X       \\
    P  & = K \cdot [R|X_0] \\
    P  & =
    \begin{bmatrix}
        c_x & 0   & x'_0 \\
        0   & c_y & y'_0 \\
        0   & 0   & 1
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        r_11 & r_21 & r_31 & X_0 \\
        r_12 & r_22 & r_32 & Y_0 \\
        r_13 & r_23 & r_33 & Z_0 \\
    \end{bmatrix}
\end{aligned}$$

Um die Beziehung zwischen zwei Bildern aufzustellen, kann man die Abbildungsgleichung nutzen. Da es hier nur um die Beziehung zwischen zwei Bildern geht, kann die Rotation und Translation des ersten Bildes auf 0 gesetzt werden (R ist dann eine 3x3-Einheitsmatrix und X₀ ein Nullvektor). X₀ des zweiten Bildes wird zur Translation zwischen den beiden Bildern.

Rückwärtsschnitt

Die Koordinaten der ArUco-Marker sind aus einer Kalibrierung und aus vorherigen Messungen bekannt. Aus diesen und den Bildkoordinaten der Marker kann die Position und Ausrichtung der Kamera berechnet werden. Hierfür wird der sogenannte Rückwärtsschnitt genutzt. Die Berechung erfolgt auf Basis der Abbildungsgleichung, die die Position eines Punktes in einem Bild in Beziehung zur Kamera setzt. Für die Berechnung selbst gibt es verschiedene Methoden. Verwendet wurde hier der von OpenCV genutzte Ansatz von in Kombination mit einem RANSAC-Ansatz. Durch die Nutzung des RANSAC-Ansatzes können veränderte Passpunkte identifiziert und als Ausreißer markiert werden.

Vorwärtsschnitt

Die Koordinaten der Passpunkt-Ausreißer aus der Berechung der Kamerapositionen werden anschließend neu berechnet. Hierfür wird der Vorwärtsschnitt genutzt. Auch hier wird OpenCV zur Berechnung genutzt. Die Berechnung der Passpunkte erfolgt für jedes Bildpaar einzeln. Für alle Koordinaten wird dann pro Passpunkt der Z-Score berechnet. Passpunkte, die einen Z-Score von über 2 haben, werden als Ausreißer markiert und nicht weiter betrachtet.

Mit dem Vorwärtsschnitt können auch die Neupunkte, die mittels SIFT oder ähnlichen Bilderkennungsalgorithmen erkannt wurden, berechnet werden. Dadurch kann schon eine dünne Punktwolke erzeugt werden, die dann in der Bündelblockausgleichung weiter optimiert werden kann.

Bündelblockausgleichung

Mittels Bündelblockausgleichung können die grob mit den vorher genannten Verfahren bestimmten Positionen und Drehungen in einer Ausgleichung optimiert werden. Hierzu gehen alle Parameter der Bilder und die Positionen der Passpunkte in die gemeinsame Ausgleichung ein. Grundlage der Ausgleichung ist die in [ss:abbildungsgleichung] beschriebene Abbildungsgleichung. Als Ergebnis erhält man die ausgeglichenen Parameter und Genauigkeitsangaben für diese.

Multi-view Stereo

Die dünne Punktwolke aus dem Vorwärtsschnitt kann durch Multi-view Stereo-Verfahren zu einem 3D-Modell erweitert werden. Hierbei werden jeweils Bildpaare gebildet und die Disparitäten, also die Verschiebung des Objektes in der Abbildung, bestimmt. Diese sind abhängig von der Entfernung des Objektes, bei einem unendlich weit entfernten Objekt tritt keine Disparität auf . Die Disparitäten können dann in Tiefeninformationen umgerechnet werden. Diese können dann wiederum gemittelt und zu einen Tiefenbild zusammengefasst werden.

Mesh-Generierung

Bis zu diesem Schritt besteht das Modell nur aus einzelnen Punkten, die keine Oberfläche ergeben. Um ein 3D-Modell zu erhalten, muss eine Oberfläche generiert werden. Hierfür wird ein Mesh-Generierungsverfahren genutzt. Hierbei wird die Punktwolke in Dreiecke unterteilt. Hierfür gibt es verschiedene Verfahren, die sich in der Art der Unterteilung unterscheiden. OpenDroneMap nutzt beispielsweise die Screened Poisson Surface Reconstruction.

Die Screened Poisson Surface Reconstruction ist ein Verfahren, das auf der Poisson-Gleichung basiert. Diese wird genutzt, um die Oberfläche zu glätten und zu interpolieren. Ein Teil des Ansatzes ist es, dass die Ausrichtung der Punkte berückstichtigt wird. Die Punkte werden hierbei in eine Gitterstruktur überführt und die Oberfläche durch die Lösung der Poisson-Gleichung bestimmt.

Texturierung

Abschließend wird das Modell texturiert. Hierfür werden die Bilder, die zur Erstellung des Modells genutzt wurden, auf das Modell projiziert. Außerdem werden Helligkeits- und Farbunterschiede ausgeglichenen.

Aufbau des Messsystemes

Die Kameras sollten eine hohe geometrische Auflösung und möglichst stabile innere Orientierung aufweisen. Außerdem sollen sie während einer Messkampagne nicht in ihrer Lage zueinander verändert werden, damit die äußere Orientierung größtenteils unverändert bleibt. Daher ist ein stabiler Rahmen notwendig, an welchem die Kameras verdrehsicher angebracht werden können. Kleinere Restfehler in den Orientierungen können mit ausgeglichen werden. Um Ungenauigkeiten durch Bewegungen zu verhindern, müssen die Kameras möglichst zeitgleich auslösen. Daher ist eine gemeinsame Steuerung und Kommunikation zwischen den Kameras notwendig. Außerdem sollen alle Bilder dann auf das Steuerungssystem übertragen werden, hierfür wir eine Form der Datenübertragung benötigt. Damit die Bilder möglichst schattenfrei ausgeleuchtet werden, muss Beleuchtung mit eingeplant werden. Außerdem muss die Stromversorgung der einzelnen Kameras sichergestellt sein.

Aus diesen Anforderungen ergeben sich die einzelnen Abschnitte dieses Kapitels.

Kameras

Als Kameras wurde das Raspberry Pi Camera Module 3 verwendet, welches jeweils von einem Raspberry Pi Zero W gesteuert wird. Im Vergleich zu anderen günstigen Kameras wie Webcams oder der ESP32 CAM haben die Kameras eine hohe geometrische Auflösung von 12 Megapixeln und dennoch mit 1, 4 μm relativ große Pixel , was im subjektiven Eindruck eine sehr gute Bildqualität ergibt.

Nachteil und Vorteil zugleich ist, dass die Kamera über einen Autofokus verfügt, der aber auch elektronisch gesteuert manuell fokussieren kann. Dieser verschlechtert die Stabilität der inneren Orientierung (vgl. [s:innereorientierung]) weiter und wurde daher auch besonders im analysiert. Da die Bilder aber Nahbereich zwischen 20 und 70 cm benötigt werden, ist hier die Schärfentiefe niedrig. Der elektronische Fokus, eine wiederholgenaue und damit mathematisch modellierbare Fokussierung vorausgesetzt, ermöglicht hier, Fokusstacking zu nutzen um den Schärfebereich zu vergrößern.

Weiterer Vorteil der Lösung mit einzelnen Raspberry Pis ist es, dass hierdurch bereits die einzelnen Kameraeinheiten Berechnungen wie das Identifizieren von Passpunkten übernehmen könnten und auch durch die Nutzung von Netzwerkverbindungen für die Steuerung das System skalierbar im Sinne der Anzahl der Kameras aber auch der Größenordnung der Abstände zwischen den Kameras.

Rahmen

[]

Kamera-Winkel

[]

Aluminum-Rahmen

Rahmen und Kameramontage-Winkel

Der Rahmen muss möglichst stabil sein, damit die Kameras sich nicht in ihrer Lage verändern können. Jedoch sollte das System auch weiterhin transportabel - also nicht zu schwer - und veränderbar bleiben, beispielsweise Kameras für Messreihen in ihrer Lage verändert werden. Der Aufbau aus genormten Bauteilen bietet sich an, um hier ggf. den Nachbau einfach ermöglichen zu können. Außerdem sollte der Rahmen auch gegebenenfalls auseinandernehmbar sein, damit er transportiert werden kann.

Als mögliche Materialien kamen Holz, Stahl und Aluminum in Frage. Aufgrund der einfachen Bearbeitung und der genormten Profile, wurde sich für Aluminiumprofile entschieden. Diese gibt es in verschiedenen Ausführungen mit Nuten an den Seitenflächen, so dass eine einfache Montage, aber auch eine Demontage zu Transportzwecken, möglich wird. Außerdem sind diese sehr stabil bei leichtem Gewicht. Durch eine Konstruktion mit Eckwürfeln sowie dem Einbau von dreieckigen Strukturen und Platten, die Scheibenwirkung haben, wurde die Stabilität der Verbindungen erhöht. [img:alurahmen] zeigt den fertigen Rahmen vor Einbau der Platten und der Technik.

Die Kameras wurden mit einem 90^(∘)-Winkel montiert, um diese weiterhin noch vertikal schwenken zu können. [img:aluwinkel] zeigt einen der Winkel, der an den Aluprofilen befestigt ist (noch ohne entsprechende Befestigungsbohrungen).

Beleuchtung

[]

Schattenarme Beleuchtung durch LED-Streifen

[]

Farbige Beleuchtung zur Statusmeldung

Beleuchtung

Um möglichst gute Bilder zu erzeugen, sollte das Objekt gut ausgeleuchtet sein. Eine dunkle Umgebung verlängert die Belichtungszeit, wodurch die Gefahr von unscharfen Aufnahmen steigt. Dunkele Bereiche (ungleichmäßige Ausleuchtung) verursachen verstärktes Rauschen in diesen Bildbereichen. Daher soll das System eine gleichmäßige Ausleuchtung ermöglichen. Problematisch ist hierbei, dass die Kameras ggf. auch die Lichtquellen mit im Bildbereich haben können, wodurch Linsenreflexionen oder Ausbrennen der Bildbereiche möglich sind. Außerdem störend sind fremde Lichtquellen, die Schatten werfen oder die Belichtung der Kameras beeinflussen können.

Es wurde sich zur Beleuchtung für einzeln steuerbare LED-Lichtstreifen entschieden (siehe [img:led_streifen]). Diese können einfach an den Aluprofilen montiert werden und ermöglichen es, einzelne Bereiche und so ggf. blendende Bereiche abzuschalten. Außerdem können hiermit auch verschiedene Lichtfarben eingestellt werden, um Statusmeldungen zu geben (siehe [img:led_farbig]) oder ggf. die Farbgebung des Objektes zu beeinflussen. Die Steuerung erfolgt über einen Raspberry Pi 4, der auch die Steuerung der Kameras übernimmt.

Für diffuseres Licht von außen kann ein halbtransparenter, weißer Stoff über den Rahmen gespannt werden. Dieser sorgt für eine gleichmäßigere Ausleuchtung durch Reflektion im inneren, verhindert auch Reflexionen an Glasscheiben und ähnlichem und vermindert Blendwirkungen durch externe Lichtquellen. Hierfür wurde aus weißem Baumwollstoff eine entsprechende Haube genäht, welche durch zwei mittels Reißverschluss verschließbaren Eingriffmöglichkeiten weiterhin das Einlegen von Objekten durch die Seite oder das Verstellen von Kameras ermöglicht.

Stromversorgung

[]

Elektroverteilung zu den einzelnen Raspberry Pi Zero

Die Raspberry Pis werden mit 5 Volt betrieben. Der Raspberry Pi Zero mit Kamera hatte dabei in Messungen einen maximalen Stromverbrauch von 270 mA aufgezeigt, der Raspberry Pi 4 kann bis zu 1,5 A unter Last verbrauchen. Hieraus ergibt sich ein Gesamtstromverbrauch von maximal rund 8 Ampere. Für den Raspbery Pi 4 wurde ein eigenes Netzteil eingeplant und für die Zero W ein gemeinsames 35 Watt-Netzteil. Versuche zeigten jedoch, dass der Stromverbrauch kurzfristig höher ausfallen konnte, so dass die Zero W, die am meisten von Spannungsabfällen betroffen sind zum Absturz gebracht wurden, wenn alle Kameras gleichzeitig auslösten. Nach dem die Last auf sicherheitshalber auf zwei weitere Netzteile verteilt wurde, lief das System zuverlässig.

Als Kabelmaterial wurde Klingeldraht mit 0, 75 mm² verwendet. Der relativ hohe Kabelquerschnitt soll für einen geringen Spannungsabfall sorgen. Durch die Verwendung von mehreren Netzteilen ist dieser jedoch nun nicht mehr notwendig. Hier würde sich nun ein geringerer Querschnitt anbieten, auch um eine einfachere Verbindung zu den Raspberry Pi Zero W zu ermöglichen. Diese wurden auf Seiten der Zero W verlötet und in den Verteilerdosen mit Federkraftklemmen verbunden (siehe [img:stromverteilung]).

Die Stromversorgung der Beleuchtung erfolgt über ein 12-V-Netzteil mit 3, 5 A Ausgangsleistung. Auch hier wurde Klingeldraht zur Verteilung zwischen den einzelnen Holmen genutzt.

Beim WLAN-Router war ein entsprechendes USB-Netzteil mitgeliefert.

Kommunikation und Datenübertragung

Die Kommunikation zwischen den Raspberry Pis erfolgt über WLAN. Vorteil dieser Lösung ist, dass hier keine weiteren Leitungen außer der Stromversorgung zu den einzelnen Raspberry Pi Zero W benötigt wird und es auch möglich wäre, die gleiche Hard- und Software auch für ein größeres System ohne Änderungen zu nutzen. Nachteilig ist die Verbindungsgeschwindigkeit, gerade im Hinblick auf die Synchronisierung der Kameras. Diese Problematik soll aber durch entsprechende Programmierung der Software möglichst klein gehalten werden.

Als weitere Datenleitungen wird eine Steuerleitung für die LED-Streifen benötigt. Über diese erfolgt die Steuerung der einzelnen LED-Gruppen. Auch hier wurde wieder Klingeldraht verwendet.

Voruntersuchungen

Vor und während des Aufbaues des eigentlichen Messystemes wurden einige Voruntersuchungen durchgeführt. Diese dienten dazu, die Machbarkeit des Systemes zu prüfen und die notwendigen Schritte zu ermitteln und zu optimieren. Hauptsächlich ging es hierbei um die Ermittlung der Kamerakonstanten und der Verzeichnung der Kamera. Auch wurde die Möglichkeit der Erstellung eines 3D-Modells durch Fokusstacking wurde untersucht. Die einzelnen Untersuchungen werden im Folgenden kurz vorgestellt.

Änderung der Kamerakonstante durch Fokussierung

These

Die Kamerakonstante einer Kamera ändert sich durch die Fokussierung. Bei gleich eingestellter Objektdistanz ist die Kamerakonstante näherungsweise gleich sein.

Ziel

In wird eine Formel (siehe [eq:kraus_fokus]) für die Bildweite b in Abhängigkeit des Gegenstandsweite g gegeben. Die Bildweite entspricht näherungsweise der Kamerakonstante . Die Nutzung der Formel als Näherungswert soll überprüft werden und eine optimierte Formel für die Raspberry Pi Kamera ermittelt werden.

$$\begin{aligned}
    \frac{1}{f} = \frac{1}{g} + \frac{1}{b}
    \label{eq:kraus_fokus}
\end{aligned}$$

Vorgehen

Die Änderungen wurden in einem Versuch beobachtet. Hierzu wurde der Raspberry Pi Zero mit montierter Kamera fest vor einem Charuco-Kalibriermuster platziert. Dieser ermöglicht auch bei unscharfen Bildern noch eine gute automatische Erkennung der zu beobachtenden Punkte. Es wurden je 11 Bilder mit unterschiedlichen Fokussierungen von 2 m bis 10 cm aufgenommen. Dieser Vorgang wurde insgesamt viermal wiederholt, um auch die Wiederholungsgenauigkeit zu ermitteln. Die Bilder wurden anschließend mit einem Python-Script unter Nutzung von OpenCV ausgewertet. Hierbei wurde die relative Veränderung der Kamerakonstante der Kamera ermittelt und mit dem erwarteten Wert verglichen. Die relativen Änderungen wurden auf eine Fokusdistanz von 20 cm (entspricht einer Kamerakonstante von 5 dpt) normiert. Relative Angaben wurden genutzt, da noch keine Kamerakonstante zu dem Zeitpunkt vorlag oder angenommen werden sollte.

Ergebnis

Die Ergebnisse sind in [img:fokus_faktor] dargestellt. Es zeigt sich, dass die Änderungen der Kamerakonstante durch die Fokusierung linear zu der Dioptrienzahl (Kehrwert der Gegenstandsweite) sind. Ein lineares Verhältnis ergibt sich auch mit der im Datenblatt angegebenen Brennweite von 4, 74 mm, jedoch eine etwas flachere Gerade (blau). Die ausgleichende Gerade (blau) ergab eine Brennweite von 6, 97 mm. Die Abweichung von 2, 23 mm entspricht einer Abweichung von 47%, was als unrealistisch hoch eingeschätzt wird. Hier scheinen sich weitere Effekte bemerkbar zu machen, die noch nicht berücksichtigt wurden. Es wurde daher auch die Verzeichnung weiter untersucht.

[]

Box-Whisker-Plot der relativen Veränderung der Kamerakonstante normalisiert auf eine Fokusdistanz von 20 cm (5 dpt)

Änderung der Verzeichnung mittels Charuco-Board

These

Durch die Verschiebung der Linsen beim Fokusieren verändert sich auch die Verzeichnung der Kamera.

Ziel

Die Veränderung der Verzeichnung soll ermittelt und eine Korrekturformel ermittelt werden.

Vorgehen

Der Versuchsaufbau von der Bestimmung der relativen Änderung der Kamerakonstante blieb bestehen. Es wurden jedoch zusätzlich die Verzeichnungen der Bilder ermittelt. Die Verzeichnung wurde mit OpenCV ermittelt und mit der erwarteten Verzeichnung verglichen.

Ergebnis

Die Ergebnisse waren nicht zufriedenstellend. Es zeigte sich, dass die Verzeichnung mit nur einer Aufnahme pro Fokussierung nicht zufriedenstellend modelliert werden konnte.

Bestimmung des Zusammenhang zwischen innerer Orientierung und Fokussierung

These

Die innere Orientierung ist von der Fokussierung abhängig.

Ziel

Es soll eine Formel zur Bestimmung von Näherungswerten für die innere Orientierung in Abhängigkeit der Fokussierung ermittelt werden.

Vorgehen

Es wurden mit fünf verschiedenen Fokussierungen mit jeweils 24 Raspberry-Pi-Kameras Bilder aufgenommen und die Bilder in Agisoft Metashape mittels Aruco-Marker orientiert, dessen Position bekannt war (siehe [sec:kalibrierung]). Außerdem wurden etwa 100 Schneider-Marker im Bildbereich der Kameras verteilt und als Verknüpfungspunkte benutzt. Es wurde jeweils in Metashape alle Bilder mit der gleichen Fokussierung als eine Kamera angenommen und die innere Orientierung bestimmt. Anschließend wurden alle Kameras nochmal einzeln ausgeglichen. Die innere Orientierung wurde in Form von Brennweite, Bildhauptpunktverschiebung und Verzeichnung ermittelt. Die Ergebnisse wurden in einem Box-Whisker-Plot dargestellt und eine ausgleichende Gerade ermittelt.

Ergebnis

Die Ergebnisse sind in [img:naeherungswerte] dargestellt. Wie auch schon in der vorherigen Untersuchung zeigt sich, dass die Brennweite linear zur Fokussierung ist. Bei der Bildhauptpunktverschiebung und der Verzeichnung ist die Abhängigkeit nicht eindeutig. [tab:naeherungswerte_corr] zeigt die Korrelationsmatrix der Näherungswerte. Es zeigt sich, dass die Fokussierung und die Brennweite stark korreliert sind. Die Bildhauptpunktverschiebung ist nur schwach korreliert. Die Verzeichnung ist nicht korreliert. Es zeigt sich, dass die innere Orientierung durch die Fokussierung beeinflusst wird. Die Brennweite ist dabei erwartungsgemäß am stärksten betroffen.

[]

Box-Whisker-Plots und ausgleichende Gerade der inneren Orientierung in Abhängigkeit von der Fokussierung [dpt]

                  Fokus [dpt]       f       cx       cy       k1       k2       k3
  ------------- ------------- ------- -------- -------- -------- -------- --------
  Fokus [dpt]           1,000   0,901    0,032   -0,128   -0,069   -0,181    0,177
  f                             1,000   -0,010   -0,211   -0,220   -0,071    0,092
  cx                                     1,000   -0,230    0,005    0,005    0,005
  cy                                              1,000    0,031    0,023   -0,051
  k1                                                       1,000   -0,925    0,866
  k2                                                                1,000   -0,985
  k3                                                                         1,000

  : Korrelationsmatrix der Näherungswerte

3D-Modell aus Fokusstacking

These

Durch Fokusstacking kann ein besseres 3D-Modell erstellt werden.

Ziel

Es soll geprüft werden, ob durch Fokusstacking die Qualität des 3D-Modell verbessert werden kann. Hierzu wird ein 3D-Modell aus einem Fokusstacking erstellt und mit einem normalen 3D-Modell verglichen.

Vorgehen

-   Automatisierter Fokusstacking

-   keine Beachtung der festen Ausrichtung

-   Transformation über SIRF und Homographie

Software-Entwicklung

Für die Steuerung der Kameras und die anschließende Berechnung des 3D-Modelles muss ein Betriebssystem für das Kamerasystem und eine entsprechende Schnittstelle zu einer SfM-Software geschaffen werden. Diese Entwicklung erfolgte hauptsächlich in Python in Form von Prototyping. Das Kapitel beschreibt die Anforderungen an die Software in [sec:Anforderungsanalyse]. Anschließend werden hieraus die Anwendungsfälle ([sec:Anwendungsfallmodellierung]) erarbeitet und abschließend die Implementation ([sec:Implementierung]) beschrieben.

Anforderungsanalyse

Funktionale Anforderungen

-   Die Kameras sollen zeitgleich auslösbar sein. Die Auslösung soll möglichst verzögerungsfrei erfolgen.

-   Die Steuerung soll auch unabhängig von anderen Geräten möglich sein, beispielsweise per Tastensteuerung.

-   Der Status des Systemes soll für den Nutzer erkennbar sein - auch ohne Anschluss eines Computers etc..

-   Es sollen Passpunkte automatisch gefunden und und für die Bestimmung der äußeren Orientierung genutzt werden.

-   Die Bilder sollen scharf und fokussiert sein.

Schnittstellen

-   Die Daten sollen intern gespeichert werden.

-   Eine Speicherung auf tragbaren Speichermedien wie USB-Sticks soll möglich sein.

-   Eine direkte Übertragung an SfM-Software soll möglich sein.

Nicht-funktionale Anforderungen

-   Die Erfassung soll ohne weitere Hardware möglich sein. Das System soll unabhängig von Netzwerkanschlüssen etc. sein.

-   Alle Kommunikation soll über WLAN erfolgen.

Anwendungsfallmodellierung

Entsprechend der benötigten Schritte aus [c:photogrammetrie] und [img:ablauf] wurde die Anwendungsfälle, die die Benutzeroberfläche ermöglichen soll, im Anwendungsfall-Diagramm in [img:anwendungsfall] zusammengetragen.

[]

Anwendungsfall-Diagramm

Aus den benötigten Daten wurde das Domänen-Klassendiagramm aus [img:dokladia] erzeugt. Dieses zeigt vor allem die Abhängigkeiten der einzelnen Datensätze untereinander.

[]

Domänen-Klassendiagramm

Implementierung

Die Programmierung des Systemes erfolgte iterativ. Einzelne Arbeitspakete wurden in einem Jupyter-Notebook ausprobiert und dann, wenn dieser Schritt erfolgreich war, in den Gesamtworkflow integriert. Größtenteils wurden der Python-Code objektorientiert und typisiert geschrieben.

Module auf den Raspberry Pi’s (Python)

Allgemeine Module und Bibliotheken

Es wurde, wenn möglich, auf fertige Python-Bibliotheken zurückgegriffen. Hierdurch sollte der Programmieraufwand verringert und auf bereits getesteten Code gesetzt werden. Die wichtigsten Bibliotheken sind:

OpenCV

ist eine Bibliothek für Bildbearbeitung und maschinelles Sehen. Sie ist weit verbreitet und bietet viele photogrammetrische Funktionen. Hiermit wurde beispielsweise die Detektion von Markern durchgeführt und die Näherungswerte der Kameras berechnet.

NumPy

bietet neben vielen weiteren Funktionen die Möglichkeit der Matrizenrechnung. Diese wurde für viele Berechnungen benötigt, beispielsweise für die Berechnungen der Kamera-Projektionen.

SciPy

wurde für die Berechnung der Bündelblockausgleichung verwendet. Der manuelle Ansatz mit den Formeln aus unter Nutzung von NumPy war sehr ressourcenlastig. Unter Verwendung von SciPy und der Projektionsgleichung konnte die Berechnungsdauer stark dezimiert werden.

Flask

wurde genutzt um die Weboberfläche und die Datendownloads bereitzustellen. Hiermit wurde ein Webserver aufgesetzt, der die Daten der Kameras anzeigt und die Steuerung ermöglicht.

Außerdem wurden einige allgemeine Klassen erstellt, die in allen Modulen genutzt wurden. Diese sind in [img:uml_common] dargestellt. Diese steuern allgemeine Funktionen wie das Logging und das Auslesen der Konfiguration. Außerdem legen die Interfaces die Struktur der Datenübertragung zwischen den Raspberry Pi’s fest.

[]

Common

Kamera-Steuerung

Die Raspberry Pi Zero W übernehmen die Steuerung der Kameras. Hierfür wurde ein Modul entwickelt, das die Kameras steuert, die Bilder aufnimmt und anschließend zur Verfügung stellt. Die Klassen sind in [img:uml_camera] dargestellt.

[]

Camera

Master-Steuerung

[]

Master

Desktop-Programm (Java)

[]

Screenshot der Connector-Software unter Ubuntu 24.04

[]

Connector

Systemkalibrierung

-   Kameramodellierung

-   Kamerakalibrierung

-   Kameraausrichtung

Genauigkeitsuntersuchungen

-   Kameraanzahl

-   Drehteller

-   Vergleichsmessung

-   

Ausblick und Fazit

Vor allem das Erzeugen der Näherungswerte in Vorbereitung der Bündelblockausgleichung benötigte deutlich mehr Zeit und Theorieverständnis als gedacht. Daher wurde leider nicht alle ursprünglich geplanten Features umgesetzt. Aufgrund von Krankheit und anderen Uni-Projekten konnte dann zusätzlich auch nicht so viel Zeit in der zweiten Semesterhälfte in das Projekt gesteckt werden, wie eigentlich ursprünglich gedacht. Es sind bisher beispielsweise keine Nebenbedingungen möglich - ein Festlegen eines Maßstabes aufgrund einer bekannten Strecke ist so nicht möglich und auch nicht die Optimierung der Ausrichtung durch die Angabe gleich hoher Punkte. Auch wird aktuell nur die Position, nicht jedoch die bereits errechnete Drehung in den EXIF-Daten gespeichert. Hierfür müsste noch eine Umrechnung der Drehung aus dem System der ECEF-Koordinaten in die für EXIF-Daten übliche Ausrichtung an der Lotrichtung der Ortes des Bildes erfolgen. Ein weiteres offenes Problem ist die bereits erwähnte Transformation des lokalen Koordinatensystemes. Hier müsste noch die Ausgleichung so optimiert werden, dass nur ein Maßstab und keine Scherung verwendet wird.

Neben den erwähnten fehlenden Funktionen wäre als weitere Erweiterungen eine Berechnung einer dichten Punktwolke denkbar. Entsprechende Bibliotheken wurden während der Entwicklung entdeckt und schienen relativ leicht einbaubar. So würde die Software zu einer Komplettlösung für SfM-Punktwolken aus Bildern werden.

Im Gesamten sorgte das Projekt dafür, ein tiefergehendes Verständnis von Photogrammmetrie im Allgemeinen und SfM im Speziellen zu erarbeiten sowie vor allem die Probleme und Schwierigkeiten kennenzulernen.

Bedienungsanleitung

Zweck

Ziel des Systemes ist es, 3D-Modelle von Objekten bis zu einer Größe von 40 cm Durchmesser zu erstellen. Die Bedienung soll dabei möglichst einfach und selbsterklärend sein, um auch Laien die Möglichkeit zu geben, das System zu bedienen.

Inbetriebnahme

Beim Aufstellen ist darauf zu achten, dass sich keine stärkeren seitlichen Lichtquellen um das System herum befinden, wie beispielsweise auch Fensterflächen. Diese könnten die Belichtung der Bilder beeinflussen und so die Qualität der 3D-Modelle negativ beeinflussen. Gegebenenfalls muss für Verschattung oder Streuung gesorgt werden.

Die Berechnung des 3D-Modelles erfolgt auf einem externen Rechner. Hier kann wahlweise Agisoft Metashape oder OpenDroneMap (NodeODM) genutzt werden. Die entsprechende Software sowie eine Java-Laufzeitumgebung müssen auf dem Rechner installiert sein und die Bilder müssen auf diesen übertragen werden. Die Übertragung kann automatisch über eine Netzwerkverbindungen oder manuell per USB-Stick erfolgen. Die mitgelieferte Verbindungssoftware muss auf dem Rechner gestartet sein (siehe [sec:SoftwareEinrichtung]).

Das System startet bei Anschluss an eine Stromversorgung selbstständig. Da die Gefahr besteht, dass die kamerasteuernden Raspberry Pi Zero Daten verlieren, wenn die Stromversorgung unterbrochen wird, sollte das System immer ordnungsgemäß heruntergefahren werden und auf eine zuverlässige Stromversorgung geachtet werden. Das Abschalten erfolgt durch langes Drücken auf den roten Taster. Das System fährt dann selbstständig herunter - erkennbar an dem Erlöschen der LEDs der Raspberry Pi Zero und der Beleuchtung - und die Stromversorgung kann getrennt werden.

Software-Einrichtung

Die Software zur Steuerung der Kameras und zur Übertragung der Bilder auf den Rechner ist in Java geschrieben. Sie kann unter Linux, Windows und MacOS genutzt werden. Auf dem Rechner muss entsprechend eine Java-Laufzeitumgebung installiert sein. Für die Nutzung von Metashape muss eine Lizenz vorhanden sein und neben der ausführbaren jar-Datei abgelegt werden. Für OpenDroneMap muss die Software in Form von NodeODM auf dem Rechner installiert sein. Die Verbindung zu einem Server ist nicht implementiert.

Kalibrierung

Die letzten Koordinaten der Passpunkte wird im System gespeichert - daher sollten diese möglichst nicht verändert werden. Falls diese dennoch verändert werden, kann das System einzelne Veränderungen berechnen und nutzen. Bei Änderung einer Vielzahl muss das System jedoch extern neu kalibriert werden, beispielsweise durch Bilder mit einer externen Kamera, wo durch dann die Koordinaten der Passpunkte neu bestimmt werden können.

Eine Kalibrierung mit Bordmitteln ist nicht möglich.

Durchführung

Das System wird gestartet in dem die Stromversorgung hergestellt wird. Die Kameras starten selbstständig und die Beleuchtung wird eingeschaltet. Nach kurzer Zeit sollten kurz alle LEDs grün leuchten. Falls nicht, sind einige Kameras nicht erreichbar. Problemlösungen werden im Kapitel [sec:Problembehandlung] behandelt.

Das Objekt wird mittig, ggf. auf einer Erhöhung im Rahmen positioniert.

Ab hier trennen sich die Wege je nach verwendeten System. Die Bildaufnahme erfolgt in allen Fällen durch kurzen Druck auf den grünen Taster.

[]

Screenshot der Connector-Software unter Ubuntu 24.04

mit Netzwerkverbindung

Der zu verwendende Rechner wird mit dem System per WLAN (bevorzugt) oder Netzwerkkabel verbunden. Die Verbindungssoftware wird gestartet und die IP-Adresse des Systemes angegeben werden. Standardmäßig lautet diese im WLAN 10.0.1.1, per Netzwerkkabel muss die IP von einem DHCP-Server festgelegt werden, beispielsweise in dem der angeschlossene Rechner als DHCP-Server konfiguriert wird. Unter Gnome ist dieses beispielsweise möglich, indem in den Netzwerkeinstellungen die Internetverbindung des PC freigegeben wird.

Anschließend wird die zu nutzende Software ausgewählt und die Verbindung hergestellt.

Nun können die Bilder aufgenommen werden (grüne Taste). Die Bilder werden auf den Rechner übertragen und dort in der ausgewählten Software weiterverarbeitet. Der Fortschritt ist in der rechten Hälfte der Verbindungssoftware zu erkennen.

Durchführung ohne direkte Weiterverarbeitung

Alternativ können die Bilder auch ohne angeschlossenen PC aufgenommen werden und später weiter verarbeitet werden. Die Bilder werden in allen Fällen auf dem Raspberry Pi 4, der die gesamte Steuerung übernimmt, gespeichert. Die Bilder können dann von der Website des Raspberry Pi heruntergeladen werden. Auch ist es möglich, einen USB-Stick in den USB-Port des Raspberry Pi 4 vor der Bildaufnahme zu stecken. Die Bilder werden dann auch auf den USB-Stick kopiert.

Die Verbindungssoftware unterstützt auch das Laden der Bilder aus einem lokalen Ordner, beispielsweise aus der ZIP der Website oder dem Ordner auf dem USB-Stick.

Wartung

Software-Updates können zu Inkompatibilitäten führen, daher sollten diese erstmal auf einem zusätzlichen Raspberry Pi ausprobiert werden, bevor diese auf dem System ausgerollt werden.

Fehlerbehebung

Kameras sind nicht erreichbar

Die Kameras sind nicht erreichbar, wenn die LEDs nicht grün leuchten. Dies kann verschiedene Ursachen haben. Als erstes sollte das System nochmal heruntergefahren werden durch einen langen Druck auf die rote Taste. Anschließend wird die Stromversorgung für einige Sekunden vollständig getrennt und wieder hergestellt. Das System sollte nun neu starten und die Kameras erreichbar sein.

Falls das noch nicht der Fall ist, hilft ein Blick in das Menü des WLAN-Routers. Hier sollten in der Übersicht aller Netzwerkgeräte die 24 Kameras, der steuernde Raspberry Pi 4 und das Gerät, über das der Zugriff erfolgt, aufgezählt sein. Falls das nicht der Fall ist, muss der fehlerhafte Raspberry Pi Zero an einen Display und eine Tastatur angeschlossen werden, um den Fehler zu finden.

Falls der Raspberry Pi im Netzwerk zu finden ist, kann sich per SSH mit dem Raspberry Pi verbunden werden. Zugangsdaten sind die Übersicht am Ende zu entnehmen.

Zugangsdaten

  Gerät               Zugang        Benutzer   Passwort
  ------------------- ------------- ---------- ----------------
  WLAN-Netzwerk       photobox                 photogrammetry
  Raspberry Pi 4      10.0.1.1      photo      box
  Raspberry Pi Zero   10.0.2.1-24   photo      box
  WLAN-Router         10.0.0.1                 photobox1

Teileliste

Mechanische Bauteile

  Bezeichnung                      Anzahl    Einheit   pro Einheit   Gesamtpreis
  ------------------------------- -------- --------- ------------- -------------
  Alu-Strebenprofil Nut 6 Typ B    5, 8 m     0, 1 m     0, 44 EUR    25, 52 EUR
  Eckwürfel                          8             1     5, 00 EUR    40, 00 EUR
  90-Grad-Winkel                     16            1     1, 40 EUR    22, 40 EUR
  45-Grad-Winkel                     16           10    23, 00 EUR    46, 00 EUR
  Hammerkopf-Mutter M4               40            1     1, 40 EUR    56, 00 EUR
  Zylinderschraube M4                40          100     3, 80 EUR     3, 80 EUR
  Scheibe M4                         40          100     1, 85 EUR     1, 85 EUR
  Winkelprofil 30 x 500mm           2 m          2 m    23, 96 EUR    23, 96 EUR
                                                                     219, 53 EUR

  : Mechanische Bauteile mit Preisen (Stand: September 2023)

Elektronische Bauteile

  Bezeichnung                  Anzahl    Einheit   pro Einheit    Gesamtpreis
  --------------------------- -------- --------- ------------- --------------
  Raspberry Pi Zero W            24            1    17, 90 EUR    429, 60 EUR
  Raspberry Pi Camera 3          24            1    29, 15 EUR    699, 61 EUR
  Raspberry Pi 4                 1             1    66, 80 EUR     66, 80 EUR
  RPi Zero Gehäuse + Kabel       24            1     3, 60 EUR      86, 40EUR
  Speicherkarte 32 GB            25            1     5, 95 EUR    142, 80 EUR
  LED-Streifen                   1             1    31, 10 EUR     31, 10 EUR
  Stromversorgung 12V 3,5 A      1             1    11, 70 EUR     11, 70 EUR
  Stromversorgung 5V 7 A         2             1    18, 20 EUR     36, 40 EUR
  Buchse für Netzteile           3             1     2, 30 EUR      6, 90 EUR
  Litze 2*0,75                                10     2, 50 EUR      2, 50 EUR
                                                                 1447, 01 EUR

  : Elektronische Bauteile mit Preisen (Stand: September 2023)

Erklärung
Hiermit versichere ich, dass ich die beiliegende Master-Thesis ohne fremde Hilfe selbstständig verfasst und nur die angegebenen Quellen und Hilfsmittel benutzt habe.
Wörtlich oder dem Sinn nach aus anderen Werken entnommene Stellen sind unter Angabe der Quellen kenntlich gemacht.
